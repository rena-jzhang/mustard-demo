{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59241ff3",
   "metadata": {},
   "source": [
    "### Display dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m file_path \u001b[38;5;241m=\u001b[39m files[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a DataFrame\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# display(df.head())\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Get the column names\u001b[39;00m\n\u001b[1;32m     34\u001b[0m columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Specify the directory where your CSV files are located\n",
    "directory = '/results/twoertwe/meta/'\n",
    "\n",
    "# Create an empty DataFrame to store column information\n",
    "column_info = pd.DataFrame(columns=['File', 'Columns', 'FeatNum', 'Unique Y Values'])\n",
    "\n",
    "# Create a dictionary to group files by their prefixes\n",
    "file_groups = defaultdict(list)\n",
    "\n",
    "# Iterate through files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        prefix = filename.split('_')[0]\n",
    "        \n",
    "        # Add the file to the corresponding group\n",
    "        file_groups[prefix].append(file_path)\n",
    "\n",
    "# Iterate through each group and analyze one CSV file from each group\n",
    "for prefix, files in file_groups.items():\n",
    "    # Take the first file from the group (you can modify this logic if needed)\n",
    "    file_path = files[0]\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # display(df.head())\n",
    "    # Get the column names\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    unique_y_values = df['y'].unique().tolist()\n",
    "    \n",
    "    # Add file and column information to the DataFrame\n",
    "    column_info = pd.concat([column_info, pd.DataFrame({'File': [file_path.split('_')[0]], 'Columns': [columns], 'FeatNum': [len(columns)], 'Unique Y Values':[unique_y_values]})], ignore_index=True)\n",
    "\n",
    "grouped_info_data = []\n",
    "\n",
    "# Iterate through each row in column_info\n",
    "for _, row in column_info.iterrows():\n",
    "    file = row['File']\n",
    "    columns = row['Columns']\n",
    "    \n",
    "    # Create a dictionary to store the grouped columns\n",
    "    grouped_columns = {}\n",
    "    \n",
    "    # Iterate through each column and group by prefix\n",
    "    for col in columns:\n",
    "        prefix = col.split('_')[0]\n",
    "        if prefix in grouped_columns:\n",
    "            grouped_columns[prefix].append(col)\n",
    "        else:\n",
    "            grouped_columns[prefix] = [col]\n",
    "            \n",
    "    print(grouped_columns.items())\n",
    "    \n",
    "    # Add the grouped columns and prefix to the list\n",
    "    grouped_info_data.append({'File': file, 'GroupedColumns': list(zip(grouped_columns.keys(), [len(i) for i in grouped_columns.values()]))})\n",
    "\n",
    "grouped_info = pd.DataFrame(grouped_info_data)\n",
    "merged_info = pd.merge(column_info, grouped_info, on='File')\n",
    "\n",
    "display(merged_info)\n",
    "merged_info.to_csv('metadata.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441baec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y  meta  acoustic  language  vision  ecg  eda  mocap\n",
      "0  1     4       140       457     125    0    0      0\n",
      "1  1     3       140       457     125    0    0      0\n",
      "2  1     8       140       457     125    0    0      0\n",
      "3  1     9        52       457     125    0    0      0\n",
      "4  1     3       140         0     125   54   62      0\n",
      "5  1    19       140       457     125    0    0    330\n",
      "6  1     1         0         0      49   18    8      0\n",
      "7  1     3       140       457     125    0    0      0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Columns</th>\n",
       "      <th>FeatNum</th>\n",
       "      <th>Unique Y Values</th>\n",
       "      <th>GroupedColumns</th>\n",
       "      <th>y</th>\n",
       "      <th>meta</th>\n",
       "      <th>acoustic</th>\n",
       "      <th>language</th>\n",
       "      <th>vision</th>\n",
       "      <th>ecg</th>\n",
       "      <th>eda</th>\n",
       "      <th>mocap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/results/twoertwe/meta/mosi</td>\n",
       "      <td>[y, meta_clip, meta_begin, meta_end, meta_id, ...</td>\n",
       "      <td>727</td>\n",
       "      <td>[-2.8, -2.6, -0.8, 1.6, -2.2, -3.0, -0.4, 0.8,...</td>\n",
       "      <td>[(y, 1), (meta, 4), (acoustic, 140), (language...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>457</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/results/twoertwe/meta/sewa</td>\n",
       "      <td>[y, meta_begin, meta_end, meta_id, acoustic_op...</td>\n",
       "      <td>726</td>\n",
       "      <td>[0.0055897776, 0.41614386, 0.3779384, 0.350838...</td>\n",
       "      <td>[(y, 1), (meta, 3), (acoustic, 140), (language...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>457</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/results/twoertwe/meta/tpot</td>\n",
       "      <td>[y, meta_Evidence, meta_Visual, meta_Language,...</td>\n",
       "      <td>731</td>\n",
       "      <td>[0, 3, 2, 1]</td>\n",
       "      <td>[(y, 1), (meta, 8), (acoustic, 140), (language...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>140</td>\n",
       "      <td>457</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/results/twoertwe/meta/umeme</td>\n",
       "      <td>[y, meta_arousal_audio, meta_valence_audio, me...</td>\n",
       "      <td>644</td>\n",
       "      <td>[4.7, 2.55, 3.06, 5.31, 3.31, 2.85, 3.5, 4.23,...</td>\n",
       "      <td>[(y, 1), (meta, 9), (acoustic, 52), (language,...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>52</td>\n",
       "      <td>457</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/results/twoertwe/meta/recola</td>\n",
       "      <td>[y, meta_begin, meta_end, meta_id, acoustic_op...</td>\n",
       "      <td>385</td>\n",
       "      <td>[0.024066666, -0.0049866666, 0.060946666, 0.10...</td>\n",
       "      <td>[(y, 1), (meta, 3), (acoustic, 140), (ecg, 54)...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>54</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/results/twoertwe/meta/iemocap</td>\n",
       "      <td>[y, meta_begin, meta_end, meta_arousal_T, meta...</td>\n",
       "      <td>1072</td>\n",
       "      <td>[2.0, 3.0, 3.5, 4.0, 2.5, 4.5, 1.5, 1.6667, 2....</td>\n",
       "      <td>[(y, 1), (meta, 19), (acoustic, 140), (languag...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>140</td>\n",
       "      <td>457</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/results/twoertwe/meta/vreed</td>\n",
       "      <td>[y, meta_id, ecg_Bpm, ecg_HF, ecg_Ibi, ecg_LF,...</td>\n",
       "      <td>77</td>\n",
       "      <td>[1, 0, 3, 2]</td>\n",
       "      <td>[(y, 1), (meta, 1), (ecg, 18), (eda, 8), (visi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/results/twoertwe/meta/mosei</td>\n",
       "      <td>[y, meta_begin, meta_end, meta_id, acoustic_op...</td>\n",
       "      <td>726</td>\n",
       "      <td>[0.0, 1.6666666, 1.3333334, 0.33333334, 0.6666...</td>\n",
       "      <td>[(y, 1), (meta, 3), (acoustic, 140), (language...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>457</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             File  \\\n",
       "0     /results/twoertwe/meta/mosi   \n",
       "1     /results/twoertwe/meta/sewa   \n",
       "2     /results/twoertwe/meta/tpot   \n",
       "3    /results/twoertwe/meta/umeme   \n",
       "4   /results/twoertwe/meta/recola   \n",
       "5  /results/twoertwe/meta/iemocap   \n",
       "6    /results/twoertwe/meta/vreed   \n",
       "7    /results/twoertwe/meta/mosei   \n",
       "\n",
       "                                             Columns FeatNum  \\\n",
       "0  [y, meta_clip, meta_begin, meta_end, meta_id, ...     727   \n",
       "1  [y, meta_begin, meta_end, meta_id, acoustic_op...     726   \n",
       "2  [y, meta_Evidence, meta_Visual, meta_Language,...     731   \n",
       "3  [y, meta_arousal_audio, meta_valence_audio, me...     644   \n",
       "4  [y, meta_begin, meta_end, meta_id, acoustic_op...     385   \n",
       "5  [y, meta_begin, meta_end, meta_arousal_T, meta...    1072   \n",
       "6  [y, meta_id, ecg_Bpm, ecg_HF, ecg_Ibi, ecg_LF,...      77   \n",
       "7  [y, meta_begin, meta_end, meta_id, acoustic_op...     726   \n",
       "\n",
       "                                     Unique Y Values  \\\n",
       "0  [-2.8, -2.6, -0.8, 1.6, -2.2, -3.0, -0.4, 0.8,...   \n",
       "1  [0.0055897776, 0.41614386, 0.3779384, 0.350838...   \n",
       "2                                       [0, 3, 2, 1]   \n",
       "3  [4.7, 2.55, 3.06, 5.31, 3.31, 2.85, 3.5, 4.23,...   \n",
       "4  [0.024066666, -0.0049866666, 0.060946666, 0.10...   \n",
       "5  [2.0, 3.0, 3.5, 4.0, 2.5, 4.5, 1.5, 1.6667, 2....   \n",
       "6                                       [1, 0, 3, 2]   \n",
       "7  [0.0, 1.6666666, 1.3333334, 0.33333334, 0.6666...   \n",
       "\n",
       "                                      GroupedColumns  y  meta  acoustic  \\\n",
       "0  [(y, 1), (meta, 4), (acoustic, 140), (language...  1     4       140   \n",
       "1  [(y, 1), (meta, 3), (acoustic, 140), (language...  1     3       140   \n",
       "2  [(y, 1), (meta, 8), (acoustic, 140), (language...  1     8       140   \n",
       "3  [(y, 1), (meta, 9), (acoustic, 52), (language,...  1     9        52   \n",
       "4  [(y, 1), (meta, 3), (acoustic, 140), (ecg, 54)...  1     3       140   \n",
       "5  [(y, 1), (meta, 19), (acoustic, 140), (languag...  1    19       140   \n",
       "6  [(y, 1), (meta, 1), (ecg, 18), (eda, 8), (visi...  1     1         0   \n",
       "7  [(y, 1), (meta, 3), (acoustic, 140), (language...  1     3       140   \n",
       "\n",
       "   language  vision  ecg  eda  mocap  \n",
       "0       457     125    0    0      0  \n",
       "1       457     125    0    0      0  \n",
       "2       457     125    0    0      0  \n",
       "3       457     125    0    0      0  \n",
       "4         0     125   54   62      0  \n",
       "5       457     125    0    0    330  \n",
       "6         0      49   18    8      0  \n",
       "7       457     125    0    0      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "    # Initialize an empty dictionary to store the processed data for the row\n",
    "    processed_data = {}\n",
    "    \n",
    "    # Iterate through each cell in the row\n",
    "    for cell in row:\n",
    "        if cell:  # Check if the cell is not empty\n",
    "            column_name = cell[0]  # The first element is the column name\n",
    "            column_value = cell[1]  # Combine the other elements\n",
    "            processed_data[column_name] = column_value\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'col' is the column name\n",
    "# Apply the function to each row\n",
    "processed_rows = merged_info.GroupedColumns.apply(process_row)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "result_df = pd.DataFrame(processed_rows.tolist())\n",
    "\n",
    "# If necessary, you can fill NaN values with an appropriate value or method\n",
    "result_df = result_df.fillna(0).astype(int)\n",
    "print(result_df)\n",
    "\n",
    "merged_result = merged_info.merge(\n",
    "result_df.fillna(0), left_index=True, right_index=True)\n",
    "\n",
    "display(merged_result)\n",
    "# List of columns to remove\n",
    "columns_to_remove = ['Columns', 'FeatNum','GroupedColumns']\n",
    "\n",
    "# Remove the specified columns\n",
    "merged_result = merged_result.drop(columns=columns_to_remove)\n",
    "\n",
    "\n",
    "merged_result.to_csv('metadata.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4813c7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset recola_arousal, size: 660\n",
      "\n",
      "Loading Dataset recola_arousal, size: 180\n",
      "\n",
      "Loading Dataset recola_arousal, size: 240\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'language'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from dataset import *\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from torch.utils.data import DataLoader\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# data_loader = DataLoader(mmi_dataset, batch_size=batch_size, collate_fn = custom_collate_fn)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_dataset_split\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtest_dataset_train_val_test_overlap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/tests/test_dataset_split.py:30\u001b[0m, in \u001b[0;36mtest_dataset_train_val_test_overlap\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m train_text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[0;32m---> 30\u001b[0m     train_text\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlanguage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_dataset) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_dataset)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_dataset) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_dataset)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'language'"
     ]
    }
   ],
   "source": [
    "\n",
    "# from dataset import *\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "# from dataset import *\n",
    "# from mmidataset import custom_collate_fn\n",
    "\n",
    "\n",
    "# dataset_rootdir = '/results/twoertwe/meta/'  # Path to your dataset directory\n",
    "\n",
    "# batch_size = 32  # Set the batch size\n",
    "# dataset_name = 'sewa_valence'\n",
    "# data_type = 'test'\n",
    "\n",
    "# non_text_features = DATASET_MODALITY[dataset_name]\n",
    "\n",
    "# # Create an instance of MMIDataset\n",
    "# mmi_dataset = MMIDataset(data_type=data_type, dataset_name=dataset_name, \n",
    "#                         dataset_rootdir=dataset_rootdir, feature_list=non_text_features)\n",
    "\n",
    "# data_loader = DataLoader(mmi_dataset, batch_size=batch_size, collate_fn = custom_collate_fn)\n",
    "\n",
    "\n",
    "from tests.test_dataset_split import *\n",
    "test_dataset_train_val_test_overlap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5bb98",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def video_processor(video_path):\n",
    "    # Load pretrained ResNet-152 model\n",
    "    model = models.resnet152(pretrained=True)\n",
    "    model = torch.nn.Sequential(*(list(model.children())[:-1])) # Remove the last fully connected layer\n",
    "    model.eval()\n",
    "\n",
    "    # Video frame extraction and preprocessing\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = preprocess_frame(frame)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) == 0:\n",
    "        return torch.empty(0, 2048)  # Return an empty tensor if no frames are extracted\n",
    "\n",
    "    # Convert list of frames to tensor\n",
    "    frames_tensor = torch.stack(frames)\n",
    "    print('frame', frames_tensor)\n",
    "\n",
    "    # Feature extraction\n",
    "    with torch.no_grad():\n",
    "        features = model(frames_tensor)\n",
    "    print('features', features)\n",
    "\n",
    "    features_mean = torch.mean(features, dim=0).unsqueeze(0)\n",
    "\n",
    "    # Flatten features from [N, 2048, 1, 1] to [N, 2048]\n",
    "    features_flattened = torch.flatten(features_mean, start_dim=1)\n",
    "\n",
    "    return features_flattened\n",
    "\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return preprocess(frame)\n",
    "\n",
    "\n",
    "def process_video_data(videos_folder, video_features_path):\n",
    "    video_features = []\n",
    "\n",
    "    for filename in os.listdir(videos_folder):\n",
    "        video_path = os.path.join(videos_folder, filename)\n",
    "        video_feature = video_processor(video_path)\n",
    "        video_features.append(video_feature)\n",
    "            \n",
    "    with open(video_features_path, 'wb') as f:\n",
    "        pickle.dump(video_features, f)\n",
    "\n",
    "    print(f\"Video features saved to {video_features_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281bc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa torch numpy\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_librosa_features(path: str) -> np.ndarray:\n",
    "    y, sr = librosa.load(path)\n",
    "\n",
    "    hop_length = 512  # Set the hop length; at 22050 Hz, 512 samples ~= 23ms\n",
    "\n",
    "    # Remove vocals first\n",
    "    D = librosa.stft(y, hop_length=hop_length)\n",
    "    S_full, phase = librosa.magphase(D)\n",
    "\n",
    "    S_filter = librosa.decompose.nn_filter(S_full, aggregate=np.median, metric=\"cosine\",\n",
    "                                           width=int(librosa.time_to_frames(0.2, sr=sr)))\n",
    "\n",
    "    S_filter = np.minimum(S_full, S_filter)\n",
    "\n",
    "    margin_i, margin_v = 2, 4\n",
    "    power = 2\n",
    "    mask_v = librosa.util.softmask(S_full - S_filter, margin_v * S_filter, power=power)\n",
    "    S_foreground = mask_v * S_full\n",
    "\n",
    "    # Recreate vocal_removal y\n",
    "    new_D = S_foreground * phase\n",
    "    y = librosa.istft(new_D)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Compute MFCC features from the raw signal\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)  # And the first-order differences (delta features)\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "    S_delta = librosa.feature.delta(S)\n",
    "\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(S=S_full)\n",
    "\n",
    "    audio_feature = np.vstack((mfcc, mfcc_delta, S, S_delta, spectral_centroid))  # combine features\n",
    "\n",
    "    # binning data\n",
    "    jump = int(audio_feature.shape[1] / 10)\n",
    "    return librosa.util.sync(audio_feature, range(1, audio_feature.shape[1], jump))\n",
    "\n",
    "def extract_audio_features(audio_file_path: str) -> torch.Tensor:\n",
    "    # Extract audio seq features using librosa\n",
    "    features = get_librosa_features(audio_file_path).T\n",
    "    \n",
    "    # avg\n",
    "    tensor_features = torch.tensor(features).mean(dim=0).unsqueeze(0)\n",
    "    return tensor_features\n",
    "\n",
    "def process_audio_data(audio_folder, audio_features_path):\n",
    "    audio_features = []\n",
    "\n",
    "    for filename in tqdm(os.listdir(audio_folder), desc=\"Processing audio files\"):\n",
    "        audio_path = os.path.join(audio_folder, filename)\n",
    "        if os.path.isfile(audio_path):\n",
    "            audio_feature = extract_audio_features(audio_path)\n",
    "            audio_features.append(audio_feature)\n",
    "\n",
    "    # Save to a pickle file\n",
    "    with open(audio_features_path, 'wb') as f:\n",
    "        pickle.dump(audio_features, f)\n",
    "    print(f\"Audio features saved to {audio_features_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined dataset paths configuration\n",
    "DATASET_PATHS = {\n",
    "    'datasets/MOSI_small': {\n",
    "        'videos_folder': '/projects/dataset_original/datasets/MOSI_small/Base_data/Videos_Segmented',\n",
    "        'audios_folder': '/projects/dataset_original/datasets/MOSI_small/Base_data/Audio_Segmented'\n",
    "    }\n",
    "\n",
    "    # Add more datasets here if needed\n",
    "}\n",
    "def prepare_dataset_paths(dataset_name):\n",
    "\n",
    "    # Check if the dataset is defined in the configuration\n",
    "    if dataset_name in DATASET_PATHS:\n",
    "        paths = DATASET_PATHS[dataset_name]\n",
    "\n",
    "        # Process video and audio data if their paths are available\n",
    "        if 'videos_folder' in paths:\n",
    "            video_features_path = f'{dataset_name}/video_features.pkl'\n",
    "            process_video_data(paths['videos_folder'], video_features_path)\n",
    "\n",
    "        if 'audios_folder' in paths:\n",
    "            audio_features_path = f'{dataset_name}/audio_features.pkl'\n",
    "            process_audio_data(paths['audios_folder'], audio_features_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset_name} not found in dataset paths\")\n",
    "\n",
    "# Example usage\n",
    "dataset_name = 'datasets/MOSI_small'\n",
    "# prepare_dataset_paths(dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text and label\n",
    "# !pip install pandas --force-reinstall\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "TEXT_FEATURE_PATH = f'{dataset_name}/text_n_label.csv'\n",
    "\n",
    "csv_file_path = TEXT_FEATURE_PATH\n",
    "\n",
    "def read_text_files(folder_path, df):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    filenames = [filename for filename in sorted(os.listdir(folder_path)) if filename.endswith('.txt')]\n",
    "    \n",
    "    print(filenames[:5])\n",
    "    # Iterate over all files in the given folder\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Check if it's a file and not a directory\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                # Read the content and add it to the list\n",
    "                text_list.append(file.read())\n",
    "                \n",
    "        label_list.append(df[df['filename'] == filename.split('.')[0]]['label'][0])\n",
    "\n",
    "    label_list = [f\"{float(number):.2f}\" for number in label_list]\n",
    "\n",
    "    return text_list, label_list\n",
    "\n",
    "\n",
    "def text_label_creation(text_list, label_list, csv_file_path = TEXT_FEATURE_PATH):\n",
    "\n",
    "    assert len(text_list) == len(label_list), \"Text and label lists must have the same length.\"\n",
    "\n",
    "    # Writing to csv file\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "\n",
    "        # Write the header\n",
    "        csvwriter.writerow(['text_input', 'label'])\n",
    "\n",
    "        # Write the data\n",
    "        for text, label in zip(text_list, label_list):\n",
    "            csvwriter.writerow([text, label])\n",
    "\n",
    "    print(f\"CSV file saved to {csv_file_path}\")\n",
    "    \n",
    "csv_path ='/projects/dataset_original/datasets/MOSI_small/Base_data/Labels/boundaries_sentimentint_avg.csv'\n",
    "text_folder = '/projects/dataset_original/datasets/MOSI_small/Base_data/Text_Per_Segment/Final'\n",
    "\n",
    "df = pd.read_csv(csv_path, header=None)\n",
    "headers = ['c1', 'c2', 'c3', 'filename', 'label']\n",
    "df.columns = headers\n",
    "\n",
    "text_list, label_list = read_text_files(text_folder, df)\n",
    "\n",
    "\n",
    "print(text_list[:5], label_list[:5])\n",
    "# text_label_creation(text_list, label_list, csv_file_path = TEXT_FEATURE_PATH)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654e5fd",
   "metadata": {},
   "source": [
    "### Result post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Calculate CCC, RMSE, and PCC.\n",
    "    :param true_values: Array of true values\n",
    "    :param predicted_values: Array of predicted values\n",
    "    :return: Concordance Correlation Coefficient, Root Mean Squared Error, Pearson Correlation Coefficient\n",
    "    \"\"\"\n",
    "    # Convert non-numeric values to NaN\n",
    "    true_values = pd.to_numeric(true_values, errors='coerce')\n",
    "    predicted_values = pd.to_numeric(predicted_values, errors='coerce')\n",
    "    \n",
    "    # Remove or impute NaNs (or use np.nanmean, np.nanvar, etc., to handle NaNs)\n",
    "    valid_indices = ~np.isnan(true_values) & ~np.isnan(predicted_values)\n",
    "    true_values = true_values[valid_indices]\n",
    "    predicted_values = predicted_values[valid_indices]\n",
    "\n",
    "    # Calculate CCC\n",
    "    mean_true = np.mean(true_values)\n",
    "    mean_predicted = np.mean(predicted_values)\n",
    "    var_true = np.var(true_values)\n",
    "    var_predicted = np.var(predicted_values)\n",
    "    pearson_corr, _ = pearsonr(true_values, predicted_values)\n",
    "    ccc = (2 * pearson_corr * np.sqrt(var_true) * np.sqrt(var_predicted)) / \\\n",
    "          (var_true + var_predicted + (mean_true - mean_predicted) ** 2)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "\n",
    "    # PCC is the Pearson Correlation Coefficient\n",
    "    pcc = pearson_corr\n",
    "\n",
    "    return ccc, rmse, pcc\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Path to your CSV file\n",
    "ALL_DATASETS = [\n",
    "            'umeme_arousal', \n",
    "                # 'vreed_av',\n",
    "\n",
    "                'iemocap_valence', 'iemocap_arousal',\n",
    "                # 'recola_valence', \n",
    "                # 'recola_arousal', \n",
    "                # 'sewa_valence', 'sewa_arousal', \n",
    "                # 'mosi_sentiment',\n",
    "                # 'mosei_sentiment', 'mosei_happiness',\n",
    "                ]\n",
    "\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "num_epoch = 30\n",
    "\n",
    "for task_name in ALL_DATASETS:\n",
    "    best_ccc = -1\n",
    "    best_rmse = float('inf')\n",
    "    best_pcc = -1\n",
    "\n",
    "    for i in range(num_epoch):\n",
    "        csv_file = f'/work/jingyiz4/mustard-demo/results/{task_name}/gpt2_nopretrain_0.0001_2_42_0_unfreeze/predictions_actuals_{i}.csv'\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        # Assuming the columns are named 'Actual' and 'Prediction'\n",
    "        true_values = df['Actual'].to_numpy()\n",
    "        predicted_values = df['Prediction'].to_numpy()\n",
    "\n",
    "        # Calculate metrics\n",
    "        ccc, rmse, pcc = calculate_metrics(true_values, predicted_values)\n",
    "\n",
    "        # Update best metrics if current epoch is better\n",
    "        if ccc > best_ccc:\n",
    "            best_ccc = ccc\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "        if pcc > best_pcc:\n",
    "            best_pcc = pcc\n",
    "\n",
    "    # Store best results for the task\n",
    "    results.append({\n",
    "        \"Task\": task_name,\n",
    "        \"Modeling\": 'gpt2',\n",
    "        \"Best RMSE\": best_rmse,\n",
    "        \"Best PCC\": best_pcc,\n",
    "        \"Best CCC\": best_ccc,\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42c6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the base directories\n",
    "source_base = \"/projects/dataset_processed\"\n",
    "target_base = \"/work/jingyiz4/datasets/\"\n",
    "# for subdir in subdirs:\n",
    "#     source_dir = os.path.join(source_base, subdir, \"twoertwein\")\n",
    "#     target_dir = os.path.join(target_base, subdir, \"twoertwein\")\n",
    "\n",
    "#     # Create the target directory if it doesn't exist\n",
    "#     os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "#     # Copy all Python files\n",
    "#     for file in os.listdir(source_dir):\n",
    "#         # if file.endswith(\".hdf\"):\n",
    "\n",
    "#         # if file.endswith(\".py\"):\n",
    "#             shutil.copy2(os.path.join(source_dir, file), target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "641fa4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sewa_valence_4_training.csv', 'sewa_arousal_0_test.csv', 'sewa_arousal_4_training.csv', 'sewa_valence_1_test.csv', 'sewa_arousal_2_validation.csv', 'sewa_valence_3_training.csv', 'sewa_arousal_0_training.csv', 'sewa_arousal_1_training.csv', 'sewa_valence_2_training.csv', 'sewa_arousal_3_training.csv', 'sewa_valence_2_validation.csv', 'sewa_valence_0_training.csv', 'sewa_valence_1_training.csv', 'sewa_arousal_2_training.csv', 'sewa_valence_0_test.csv', 'sewa_arousal_0_validation.csv', 'sewa_arousal_1_test.csv', 'sewa_valence_0_validation.csv', 'sewa_valence_1_validation.csv', 'sewa_valence_4_test.csv', 'sewa_arousal_2_test.csv', 'sewa_arousal_1_validation.csv', 'sewa_valence_3_test.csv', 'sewa_valence_4_validation.csv', 'sewa_arousal_4_test.csv', 'sewa_valence_3_validation.csv', 'sewa_valence_2_test.csv', 'sewa_arousal_4_validation.csv', 'sewa_arousal_3_test.csv', 'sewa_arousal_3_validation.csv']\n",
      "Dataset: SEWA\n",
      "Columns in ds_df: ['meta_begin', 'meta_end', 'German', 'arousal', 'valence', 'liking', 'name', 'sentence']\n",
      "Columns in df: ['y', 'meta_begin', 'meta_end', 'meta_id', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F0semitoneFrom27.5Hz_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F0semitoneFrom27.5Hz_sma3nz_meanFallingSlope_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F0semitoneFrom27.5Hz_sma3nz_meanRisingSlope_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F0semitoneFrom27.5Hz_sma3nz_pctlrange0-2_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F0semitoneFrom27.5Hz_sma3nz_percentile20.0_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F0semitoneFrom27.5Hz_sma3nz_percentile50.0_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F0semitoneFrom27.5Hz_sma3nz_percentile80.0_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F0semitoneFrom27.5Hz_sma3nz_stddevFallingSlope_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F0semitoneFrom27.5Hz_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F0semitoneFrom27.5Hz_sma3nz_stddevRisingSlope_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F1amplitudeLogRelF0_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F1amplitudeLogRelF0_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F1bandwidth_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F1bandwidth_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F1frequency_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F1frequency_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F2amplitudeLogRelF0_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F2amplitudeLogRelF0_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F2bandwidth_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F2bandwidth_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F2frequency_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F2frequency_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F3amplitudeLogRelF0_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F3amplitudeLogRelF0_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F3bandwidth_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F3bandwidth_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F3frequency_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_F3frequency_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_HNRdBACF_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_HNRdBACF_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_MeanUnvoicedSegmentLength_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_MeanVoicedSegmentLengthSec_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_StddevUnvoicedSegmentLength_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_StddevVoicedSegmentLengthSec_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_VoicedSegmentsPerSec_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_alphaRatioUV_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_alphaRatioV_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_alphaRatioV_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_equivalentSoundLevel_dBp_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_hammarbergIndexUV_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_hammarbergIndexV_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_hammarbergIndexV_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_jitterLocal_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_jitterLocal_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_logRelF0-H1-A3_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_logRelF0-H1-A3_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_logRelF0-H1-H2_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_logRelF0-H1-H2_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudnessPeaksPerSec_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudness_sma3_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudness_sma3_meanFallingSlope_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudness_sma3_meanRisingSlope_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudness_sma3_pctlrange0-2_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudness_sma3_percentile20.0_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudness_sma3_percentile50.0_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudness_sma3_percentile80.0_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudness_sma3_stddevFallingSlope_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudness_sma3_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_loudness_sma3_stddevRisingSlope_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc1V_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc1V_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc1_sma3_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc1_sma3_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc2V_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc2V_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc2_sma3_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc2_sma3_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc3V_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc3V_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc3_sma3_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc3_sma3_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc4V_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc4V_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc4_sma3_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_mfcc4_sma3_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_shimmerLocaldB_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_shimmerLocaldB_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_slopeUV0-500_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_slopeUV500-1500_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_slopeV0-500_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_slopeV0-500_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_slopeV500-1500_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_slopeV500-1500_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_spectralFluxUV_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_spectralFluxV_sma3nz_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_spectralFluxV_sma3nz_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_spectralFlux_sma3_amean_median', 'acoustic_opensmile_eGeMAPSv02_csvoutput_spectralFlux_sma3_stddevNorm_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F0semitoneFrom27.5Hz_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F0semitoneFrom27.5Hz_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F1amplitudeLogRelF0_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F1amplitudeLogRelF0_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F1bandwidth_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F1bandwidth_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F1frequency_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F1frequency_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F2amplitudeLogRelF0_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F2amplitudeLogRelF0_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F2bandwidth_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F2bandwidth_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F2frequency_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F2frequency_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F3amplitudeLogRelF0_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F3amplitudeLogRelF0_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F3bandwidth_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F3bandwidth_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F3frequency_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_F3frequency_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_HNRdBACF_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_HNRdBACF_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_Loudness_sma3_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_Loudness_sma3_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_alphaRatio_sma3_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_alphaRatio_sma3_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_hammarbergIndex_sma3_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_hammarbergIndex_sma3_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_jitterLocal_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_jitterLocal_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_logRelF0-H1-A3_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_logRelF0-H1-A3_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_logRelF0-H1-H2_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_logRelF0-H1-H2_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_mfcc1_sma3_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_mfcc1_sma3_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_mfcc2_sma3_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_mfcc2_sma3_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_mfcc3_sma3_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_mfcc3_sma3_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_mfcc4_sma3_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_mfcc4_sma3_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_shimmerLocaldB_sma3nz_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_shimmerLocaldB_sma3nz_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_slope0-500_sma3_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_slope0-500_sma3_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_slope500-1500_sma3_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_slope500-1500_sma3_median', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_spectralFlux_sma3_iqr', 'acoustic_opensmile_eGeMAPSv02_lldcsvoutput_spectralFlux_sma3_median', 'acoustic_opensmile_vad_opensource_csvoutput_vad_iqr', 'acoustic_opensmile_vad_opensource_csvoutput_vad_median', 'vision_duchenne_smile_ratio', 'vision_openface_AU01_c_max', 'vision_openface_AU01_c_mean', 'vision_openface_AU01_c_std', 'vision_openface_AU01_r_max', 'vision_openface_AU01_r_mean', 'vision_openface_AU01_r_std', 'vision_openface_AU02_c_max', 'vision_openface_AU02_c_mean', 'vision_openface_AU02_c_std', 'vision_openface_AU02_r_max', 'vision_openface_AU02_r_mean', 'vision_openface_AU02_r_std', 'vision_openface_AU04_c_max', 'vision_openface_AU04_c_mean', 'vision_openface_AU04_c_std', 'vision_openface_AU04_r_max', 'vision_openface_AU04_r_mean', 'vision_openface_AU04_r_std', 'vision_openface_AU05_c_max', 'vision_openface_AU05_c_mean', 'vision_openface_AU05_c_std', 'vision_openface_AU05_r_max', 'vision_openface_AU05_r_mean', 'vision_openface_AU05_r_std', 'vision_openface_AU06_c_max', 'vision_openface_AU06_c_mean', 'vision_openface_AU06_c_std', 'vision_openface_AU06_r_max', 'vision_openface_AU06_r_mean', 'vision_openface_AU06_r_std', 'vision_openface_AU07_c_max', 'vision_openface_AU07_c_mean', 'vision_openface_AU07_c_std', 'vision_openface_AU07_r_max', 'vision_openface_AU07_r_mean', 'vision_openface_AU07_r_std', 'vision_openface_AU09_c_max', 'vision_openface_AU09_c_mean', 'vision_openface_AU09_c_std', 'vision_openface_AU09_r_max', 'vision_openface_AU09_r_mean', 'vision_openface_AU09_r_std', 'vision_openface_AU10_c_max', 'vision_openface_AU10_c_mean', 'vision_openface_AU10_c_std', 'vision_openface_AU10_r_max', 'vision_openface_AU10_r_mean', 'vision_openface_AU10_r_std', 'vision_openface_AU12_c_max', 'vision_openface_AU12_c_mean', 'vision_openface_AU12_c_std', 'vision_openface_AU12_c_threshold_filtered_count', 'vision_openface_AU12_c_threshold_filtered_duration_mean', 'vision_openface_AU12_r_max', 'vision_openface_AU12_r_mean', 'vision_openface_AU12_r_std', 'vision_openface_AU12_r_threshold_filtered_count', 'vision_openface_AU12_r_threshold_filtered_duration_mean', 'vision_openface_AU14_c_max', 'vision_openface_AU14_c_mean', 'vision_openface_AU14_c_std', 'vision_openface_AU14_r_max', 'vision_openface_AU14_r_mean', 'vision_openface_AU14_r_std', 'vision_openface_AU15_c_max', 'vision_openface_AU15_c_mean', 'vision_openface_AU15_c_std', 'vision_openface_AU15_r_max', 'vision_openface_AU15_r_mean', 'vision_openface_AU15_r_std', 'vision_openface_AU17_c_max', 'vision_openface_AU17_c_mean', 'vision_openface_AU17_c_std', 'vision_openface_AU17_c_threshold_filtered_count', 'vision_openface_AU17_r_max', 'vision_openface_AU17_r_mean', 'vision_openface_AU17_r_std', 'vision_openface_AU17_r_threshold_filtered_count', 'vision_openface_AU20_c_max', 'vision_openface_AU20_c_mean', 'vision_openface_AU20_c_std', 'vision_openface_AU20_r_max', 'vision_openface_AU20_r_mean', 'vision_openface_AU20_r_std', 'vision_openface_AU23_c_max', 'vision_openface_AU23_c_mean', 'vision_openface_AU23_c_std', 'vision_openface_AU23_r_max', 'vision_openface_AU23_r_mean', 'vision_openface_AU23_r_std', 'vision_openface_AU25_c_max', 'vision_openface_AU25_c_mean', 'vision_openface_AU25_c_std', 'vision_openface_AU25_r_max', 'vision_openface_AU25_r_mean', 'vision_openface_AU25_r_std', 'vision_openface_AU26_c_max', 'vision_openface_AU26_c_mean', 'vision_openface_AU26_c_std', 'vision_openface_AU26_r_max', 'vision_openface_AU26_r_mean', 'vision_openface_AU26_r_std', 'vision_openface_AU28_c_max', 'vision_openface_AU28_c_mean', 'vision_openface_AU28_c_std', 'vision_openface_AU45_c_max', 'vision_openface_AU45_c_mean', 'vision_openface_AU45_c_std', 'vision_openface_AU45_r_max', 'vision_openface_AU45_r_mean', 'vision_openface_AU45_r_std', 'vision_openface_confidence_mean', 'vision_openface_confidence_std', 'vision_openface_euclidean_distance_pose_Rxyz_diff_abs_mean', 'vision_openface_euclidean_distance_pose_Rxyz_mean', 'vision_openface_euclidean_norm_pose_Rxyz_std', 'vision_openface_gaze_angle_x_mean', 'vision_openface_gaze_angle_x_std', 'vision_openface_gaze_angle_y_mean', 'vision_openface_gaze_angle_y_std', 'vision_openface_pose_Rx_mean', 'vision_openface_pose_Rx_std', 'vision_openface_pose_Ry_mean', 'vision_openface_pose_Ry_std']\n",
      "2553 1096\n",
      "Shared columns: ['meta_begin', 'meta_end']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta_begin</th>\n",
       "      <th>meta_end</th>\n",
       "      <th>German</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>0.879274</td>\n",
       "      <td>2.665017</td>\n",
       "      <td>&lt;laughter&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>2.665017</td>\n",
       "      <td>3.072928</td>\n",
       "      <td>Hallo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>3.072928</td>\n",
       "      <td>3.623900</td>\n",
       "      <td>&lt;laughter&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>4.586731</td>\n",
       "      <td>6.299956</td>\n",
       "      <td>Haben wir jetzt nur eine bestimmte Zeit?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>6.299956</td>\n",
       "      <td>8.674903</td>\n",
       "      <td>&lt;&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      meta_begin  meta_end                                    German\n",
       "1653    0.879274  2.665017                                <laughter>\n",
       "1654    2.665017  3.072928                                     Hallo\n",
       "1655    3.072928  3.623900                                <laughter>\n",
       "1656    4.586731  6.299956  Haben wir jetzt nur eine bestimmte Zeit?\n",
       "2468    6.299956  8.674903                                        <>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta_begin</th>\n",
       "      <th>meta_end</th>\n",
       "      <th>meta_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.120781</td>\n",
       "      <td>2.041400</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.634100</td>\n",
       "      <td>4.012500</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.625100</td>\n",
       "      <td>5.199047</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.199047</td>\n",
       "      <td>7.718600</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.718600</td>\n",
       "      <td>8.718600</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   meta_begin  meta_end  meta_id\n",
       "0    1.120781  2.041400       14\n",
       "1    2.634100  4.012500       14\n",
       "2    4.625100  5.199047       14\n",
       "3    5.199047  7.718600       14\n",
       "4    7.718600  8.718600       14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1653    0.879274_2.665017\n",
       "1654    2.665017_3.072928\n",
       "1655      3.072928_3.6239\n",
       "1656    4.586731_6.299956\n",
       "2468    6.299956_8.674903\n",
       "Name: key, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    1.120781_2.0414\n",
       "1      2.6341_4.0125\n",
       "2    4.6251_5.199047\n",
       "3    5.199047_7.7186\n",
       "4      7.7186_8.7186\n",
       "Name: key, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1653                                         laughter\n",
       "1654                                            Hello\n",
       "1655                                         laughter\n",
       "1656    Do we only have a certain amount of time now?\n",
       "2468                                                 \n",
       "Name: sentence, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0              laughter\n",
       "1    Hi, do you see me?\n",
       "2                  Okay\n",
       "3                      \n",
       "4             breathing\n",
       "Name: sentence, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     new_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_df) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(df)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# new_df.to_csv(os.path.join(new_dataset_rootdir, csv_file))\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def read_df_from_hdf(task_name):\n",
    "    file_path = f\"/work/jingyiz4/datasets/{task_name}/twoertwein/all_minilm_l12_v2.hdf\"\n",
    "\n",
    "    # Read the dataframe from the HDF file\n",
    "    try:\n",
    "        stored_df = pd.read_hdf(file_path, key='df')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the HDF file: {e}\")\n",
    "    meta_strings_filtered = [\n",
    "        string for string in stored_df.columns \n",
    "        if 'liwc_' not in string and 'all_minilm_' not in string \n",
    "    ]    \n",
    "\n",
    "    # display(meta_strings_filtered)\n",
    "    # display(stored_df[meta_strings_filtered].head(5))\n",
    "    # print(len(stored_df))\n",
    "    return stored_df[meta_strings_filtered]\n",
    "    \n",
    "\n",
    "# List of subdrectories\n",
    "datasets = {\n",
    "    # \"UMEME\": 'umeme', # done\n",
    "    # \"MOSI\": 'mosi',\n",
    "    # \"MOSEI\": 'mosei', # sentence merge issue\n",
    "    # \"AVEC16-RECOLA\": 'recola', #done\n",
    "    \"SEWA\": 'sewa',\n",
    "    # \"IEMOCAP\": 'iemocap'  # Uncomment if needed\n",
    "}\n",
    "\n",
    "dataset_rootdir = '/results/twoertwe/meta/'  # Path to your dataset directory\n",
    "new_dataset_rootdir = '/work/jingyiz4/cleaned_data/'\n",
    "\n",
    "# Assuming the HDF file is \"all_minilm_l12_v2.hdf\" and the key for the dataframe is \"df\"\n",
    "for dataset, dataset_abbr in datasets.items():\n",
    "    ds_df = read_df_from_hdf(dataset)\n",
    "\n",
    "    # List all files in dataset_dir\n",
    "    all_files = os.listdir(dataset_rootdir)\n",
    "\n",
    "    # Filter for CSV files that include 'dataset' in their name\n",
    "    csv_files = [file for file in all_files if dataset_abbr in file and file.endswith('.csv')]\n",
    "\n",
    "    print(csv_files)\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(dataset_rootdir, csv_file)\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # remove old text columns, now both dfs are cleaned\n",
    "        non_emb_col = [\n",
    "            string for string in df.columns \n",
    "            if 'liwc_' not in string and 'all_minilm_' not in string \n",
    "        ]    \n",
    "        df = df[non_emb_col]\n",
    "\n",
    "        # Append 'sentence' column from ds_df to df\n",
    "        # Ensure the 'sentence' column exists in ds_df\n",
    "        if 'sentence' in ds_df.columns:\n",
    "            # Find shared columns\n",
    "            shared_cols = list(set(ds_df.columns).intersection(set(df.columns)))\n",
    "            print(f\"Dataset: {dataset}\")\n",
    "            # shared_cols = ['meta_begin', 'meta_end']\n",
    "            print(f\"Columns in ds_df: {ds_df.columns.tolist()}\")\n",
    "            print(f\"Columns in df: {df.columns.tolist()}\")\n",
    "            print(len(ds_df), len(df))\n",
    "            \n",
    "            print(f\"Shared columns: {shared_cols}\")\n",
    "            display(ds_df[['meta_begin', 'meta_end','German']].head())   \n",
    "\n",
    "\n",
    "            display(df[['meta_begin', 'meta_end','meta_id']].head())   \n",
    "\n",
    "            print()\n",
    "\n",
    "            ds_df['key'] = ds_df[shared_cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "            df['key'] = df[shared_cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "            display(ds_df['key'].head())\n",
    "            display(df['key'].head())   \n",
    "\n",
    "            new_df = df.merge(ds_df[['key', 'sentence']], on='key', how='left')\n",
    "            new_df.drop('key', inplace=True, axis=1)\n",
    "            display(ds_df['sentence'].head())\n",
    "            display(new_df['sentence'].head())\n",
    "        else:\n",
    "            new_df = df.copy()\n",
    "\n",
    "        assert len(new_df) == len(df)\n",
    "\n",
    "        break\n",
    "        # new_df.to_csv(os.path.join(new_dataset_rootdir, csv_file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9853c88c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n           ^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/mustard-demo/dataloader.py\", line 60, in align\n    tmp.append(get_markers(interval_data))\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/mustard-demo/python_tools/ml/markers.py\", line 160, in get_markers\n    intervals[key] = index_to_segments(data[key].numpy(), data.index.to_numpy())\n                                       ^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 6204, in __getattr__\n    return object.__getattribute__(self, name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'Series' object has no attribute 'numpy'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2128559/3226777142.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# (\"vreed\", (\"av\",)),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m ):\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# get 5-fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         for fold, partition in get_partitions(\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;34mf\"{dataset}/{label}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         ).items():\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# training, validation, test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jingyiz4/mustard-demo/dataloader.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(dimension, batch_size)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_partitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0mklass_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0mklass_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RECOLA\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AVEC16_RECOLA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mklass_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m     return {\n\u001b[1;32m    521\u001b[0m         fold: {\n\u001b[1;32m    522\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jingyiz4/mustard-demo/dataloader.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminilm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"liwc_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             }\n\u001b[1;32m    113\u001b[0m         )\n\u001b[0;32m--> 114\u001b[0;31m         self.features = pd.concat([self.minilm, self.align_features()], axis=1).astype(\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         )\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meta_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"meta_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jingyiz4/mustard-demo/dataloader.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m             intervals.append(\n\u001b[1;32m    149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminilm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminilm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"meta_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"meta_end\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             )\n\u001b[1;32m    151\u001b[0m         features = pd.concat(\n\u001b[0;32m--> 152\u001b[0;31m             map_parallel(\n\u001b[0m\u001b[1;32m    153\u001b[0m                 partial(\n\u001b[1;32m    154\u001b[0m                     \u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(function, elements, workers, chunksize)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# process elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[1;32m    364\u001b[0m         \u001b[0mApply\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollecting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         '''\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mjob_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/jingyiz4/mustard-demo/dataloader.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0minterval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minterval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_markers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mnan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NaN\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnan\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/ml/markers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# prepare markers based on intervals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mintervals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{key}_threshold\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mintervals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_to_segments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# prepare markers based on filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdurations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6206\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sys.path.append(\"/results/twoertwe/emro\")\n",
    "\n",
    "from dataloader import get_partitions\n",
    "\n",
    "\n",
    "for dataset, labels in (\n",
    "    # regression\n",
    "    # (\"mosi\", (\"sentiment\",)),\n",
    "    # (\"mosei\", (\"sentiment\", \"happiness\")),\n",
    "    # (\"sewa\", (\"arousal\", \"valence\")),\n",
    "    # (\"recola\", (\"arousal\", \"valence\")),\n",
    "    # (\"iemocap\", (\"arousal\", \"valence\")),\n",
    "    (\"umeme\", (\"arousal\", \"valence\")),\n",
    "    # # classification (4 classes)\n",
    "    # (\"tpot\", (\"constructs\",)),\n",
    "    # # classification (5 classes)\n",
    "    # (\"vreed\", (\"av\",)),\n",
    "):\n",
    "    for label in labels:\n",
    "        # get 5-fold\n",
    "        for fold, partition in get_partitions(\n",
    "            f\"{dataset}/{label}\", batch_size=-1\n",
    "        ).items():\n",
    "            # training, validation, test sets\n",
    "            for name, data in partition.items():\n",
    "                assert len(data.iterator) == 1\n",
    "                features = pd.DataFrame(\n",
    "                    data.iterator[0].pop(\"x\")[0],\n",
    "                    columns=data.properties[\"x_names\"],\n",
    "                )\n",
    "                labels_metadata = pd.DataFrame(\n",
    "                    {key: value[0][:, 0] for key, value in data.iterator[0].items()}\n",
    "                )\n",
    "                final_csv = pd.concat([labels_metadata, features], axis=1)\n",
    "                display(final_csv.head())\n",
    "                break\n",
    "                # final_csv.to_csv(\n",
    "                #     f\"{dataset}_{label}_{fold}_{name}.csv\", index=False\n",
    "                # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5742196",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stored_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstored_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stored_df' is not defined"
     ]
    }
   ],
   "source": [
    "stored_df['meta_id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9482e828",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FeatureExtraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n           ^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2095786/1794286718.py\", line 22, in extract\n    features.extract_features(video=file, audio=file, caches=cache)\n  File \"/work/jingyiz4/mustard-demo/python_tools/features.py\", line 546, in extract_features\n    datum = run_openface(video, cache=caches[key], **kwargs[key])\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/mustard-demo/python_tools/caching.py\", line 190, in wrapper\n    result = function(*args, cache=cache, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/mustard-demo/python_tools/features.py\", line 411, in run_openface\n    returncode, _, error = generic.run(command, stdout=None)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/mustard-demo/python_tools/generic.py\", line 136, in run\n    with subprocess.Popen(\n         ^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/subprocess.py\", line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/subprocess.py\", line 1950, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nFileNotFoundError: [Errno 2] No such file or directory: 'FeatureExtraction'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m     features\u001b[38;5;241m.\u001b[39mextract_features(video\u001b[38;5;241m=\u001b[39mfile, audio\u001b[38;5;241m=\u001b[39mfile, caches\u001b[38;5;241m=\u001b[39mcache)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mmap_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextract\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/projects/dataset_original/UMEME/media/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.mp*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow can I not\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIm quite sure that we will find some way or another\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly I joined her in the ceremony\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     58\u001b[0m     }\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/generic.py:56\u001b[0m, in \u001b[0;36mmap_parallel\u001b[0;34m(function, elements, workers, chunksize)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m workers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mworkers) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(function, elements))\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py:48\u001b[0m, in \u001b[0;36mmapstar\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmapstar\u001b[39m(args):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;241m*\u001b[39margs))\n",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m, in \u001b[0;36mextract\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopensmile_eGeMAPSv02\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopensmile_eGeMAPSv02/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopensmile_vad_opensource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopensmile_vad_opensource/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m features\u001b[38;5;241m.\u001b[39mextract_features(video\u001b[38;5;241m=\u001b[39mfile, audio\u001b[38;5;241m=\u001b[39mfile, caches\u001b[38;5;241m=\u001b[39mcache)\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/features.py:546\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m()\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m key:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenface\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 546\u001b[0m         datum \u001b[38;5;241m=\u001b[39m run_openface(video, cache\u001b[38;5;241m=\u001b[39mcaches[key], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[key])\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovarep\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    548\u001b[0m         datum \u001b[38;5;241m=\u001b[39m run_covarep(audio, cache\u001b[38;5;241m=\u001b[39mcaches[key], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[key])\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/caching.py:190\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# run/save\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m result \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39margs, cache\u001b[38;5;241m=\u001b[39mcache, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    192\u001b[0m     write_hdfs(cache, result)\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/features.py:411\u001b[0m, in \u001b[0;36mrun_openface\u001b[0;34m()\u001b[0m\n\u001b[1;32m    405\u001b[0m command \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../OpenFace/exe/FeatureExtraction/FeatureExtraction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -out_dir \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# run OpenFace\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m returncode, _, error \u001b[38;5;241m=\u001b[39m generic\u001b[38;5;241m.\u001b[39mrun(command, stdout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    412\u001b[0m detail_file \u001b[38;5;241m=\u001b[39m output_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_of_details.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m detail_file\u001b[38;5;241m.\u001b[39munlink(missing_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/generic.py:136\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stderr \u001b[38;5;241m==\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPIPE:\n\u001b[1;32m    134\u001b[0m     stderr \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mTemporaryFile(buffering\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m    137\u001b[0m     command,\n\u001b[1;32m    138\u001b[0m     stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[1;32m    139\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr,\n\u001b[1;32m    140\u001b[0m     stdin\u001b[38;5;241m=\u001b[39mstdin,\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    144\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/subprocess.py:1950\u001b[0m, in \u001b[0;36m_execute_child\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FeatureExtraction'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from python_tools import caching, features\n",
    "from python_tools.generic import map_parallel\n",
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(0, \"/projects/dataset_processed/TPOT/twoertwein\")\n",
    "from python_tools.extract import extract_liwc, load_liwc\n",
    "\n",
    "\n",
    "def extract(file: Path) -> None:\n",
    "    name = file.with_suffix(\".hdf\").name\n",
    "    # openface+opensmile\n",
    "    cache = {}\n",
    "    if file.suffix == \".mp4\":\n",
    "        cache[\"openface\"] = Path(f\"openface/{name}\")\n",
    "    if file.suffix == \".mp3\" or \"Only\" not in file.name:\n",
    "        cache[\"opensmile_eGeMAPSv02\"] = Path(f\"opensmile_eGeMAPSv02/{name}\")\n",
    "        cache[\"opensmile_vad_opensource\"] = Path(f\"opensmile_vad_opensource/{name}\")\n",
    "    features.extract_features(video=file, audio=file, caches=cache)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    map_parallel(\n",
    "        extract,\n",
    "        Path(\"/projects/dataset_original/UMEME/media/\").glob(\"*.mp*\"),\n",
    "        workers=7,\n",
    "    )\n",
    "\n",
    "    sentences = (\n",
    "        \"How can I not\",\n",
    "        \"Im quite sure that we will find some way or another\",\n",
    "        \"Ella Jorgenson made the pudding\",\n",
    "        \"The floor was completely covered\",\n",
    "        \"They are just going to go ahead regardless\",\n",
    "        \"It has all been scheduled since Wednesday\",\n",
    "        \"I am going shopping\",\n",
    "        \"A preliminary study shows rats to be more inquisitive than once thought\",\n",
    "        \"Thats it the meeting is finished\",\n",
    "        \"I dont know how she could miss this opportunity\",\n",
    "        \"It is raining outside\",\n",
    "        \"Your dog is insane\",\n",
    "        \"She told me what you did\",\n",
    "        \"Your grandmother is on the phone\",\n",
    "        \"Only I joined her in the ceremony\",\n",
    "    )\n",
    "    data = {\n",
    "        \"file\": [],\n",
    "        \"name\": [],\n",
    "        \"sentence\": [],\n",
    "        \"valence\": [],\n",
    "        \"arousal\": [],\n",
    "        \"dominance\": [],\n",
    "        \"audio\": [],\n",
    "        \"video\": [],\n",
    "    }\n",
    "    for block in (\n",
    "        Path(\"/projects/dataset_original/UMEME/evaluation.txt\")\n",
    "        .read_text()\n",
    "        .split(\"\\n\\n\")\n",
    "    ):\n",
    "        if not block:\n",
    "            continue\n",
    "        name, *details = block.split(\"\\n\")\n",
    "        details = {\n",
    "            detail.split(\":\", 1)[0].strip(): detail.split(\":\", 1)[1].strip()\n",
    "            for detail in details\n",
    "        }\n",
    "\n",
    "        data[\"name\"].append(details[\"speaker\"])\n",
    "        match details[\"modality\"]:\n",
    "            case \"av\":\n",
    "                file = f\"{name}_original.mp4\"\n",
    "            case \"video\":\n",
    "                file = f\"{name}_videoOnly.mp4\"\n",
    "            case \"audio\":\n",
    "                file = f\"{name}_audioOnly.mp3\"\n",
    "            case _:\n",
    "                assert False, _\n",
    "        assert (Path(\"/projects/dataset_original/UMEME/media\") / file).is_file()\n",
    "        data[\"file\"].append(file)\n",
    "        data[\"sentence\"].append(\n",
    "            sentences[int(name.split(\"-\", 1)[0].split(\"S\", 1)[1][:-1]) - 1]\n",
    "        )\n",
    "        \n",
    "        # TODO\n",
    "        # data[\"arousal\"].append(float(details[\"act\"].split(\"+\", 1)[0]))\n",
    "        # data[\"valence\"].append(float(details[\"val\"].split(\"+\", 1)[0]))\n",
    "        # data[\"dominance\"].append(float(details[\"dom\"].split(\"+\", 1)[0]))\n",
    "        # data[\"audio\"].append(details.get(\"audio\", \"\"))\n",
    "        # data[\"video\"].append(details.get(\"video\", \"\"))\n",
    "\n",
    "    # copy labels of uni-modal ratings but discard their features\n",
    "    data = pd.DataFrame(data)\n",
    "    display(data.head())\n",
    "    unimodal = data.loc[data[\"file\"].apply(lambda x: \"Only\" in x)].copy()\n",
    "    unimodal[\"file\"] = unimodal[\"file\"].apply(lambda x: x.split(\".\")[0])\n",
    "    labels = (\"arousal\", \"valence\", \"dominance\")\n",
    "    for key in (\"audio\", \"video\"):\n",
    "        for label in labels:\n",
    "            data[f\"meta_{label}_{key}\"] = float(\"NaN\")\n",
    "    for index, row in data.iterrows():\n",
    "        if \"Only\" in row[\"file\"]:\n",
    "            continue\n",
    "        for key in (\"audio\", \"video\"):\n",
    "            match = unimodal[\"file\"] == f\"{row[key]}_{key}Only\"\n",
    "            if not match.any():\n",
    "                print(f\"{row[key]}_{key}Only\")\n",
    "                continue\n",
    "            assert match.sum() == 1\n",
    "            unimodal_index = match[match].index[0]\n",
    "\n",
    "            for label in labels:\n",
    "                data.loc[index, f\"meta_{label}_{key}\"] = unimodal.loc[\n",
    "                    unimodal_index, label\n",
    "                ]\n",
    "    data = data.loc[data[\"file\"].apply(lambda x: \"Only\" not in x)]\n",
    "\n",
    "    # model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "    # embeddings = pd.DataFrame(\n",
    "    #     model.encode(data[\"sentence\"].str.lower().tolist()), index=data.index\n",
    "    # )\n",
    "\n",
    "    liwc = load_liwc()\n",
    "    liwc_features = extract_liwc(liwc, data[\"sentence\"])\n",
    "\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            data,\n",
    "            # embeddings.add_prefix(\"all_minilm_l12_v2_\"),\n",
    "            liwc_features.add_prefix(\"liwc_\"),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    display(data.head())\n",
    "    # caching.write_hdfs(Path(\"all_minilm_l12_v2.hdf\"), {\"df\": data})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32312d5",
   "metadata": {},
   "source": [
    "### Create latex result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "995663e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1122501/269508892.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# df = pd.DataFrame(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Generate LaTeX table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mlatex_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_grouped_latex_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Task\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Grouped Performance Metrics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tab:grouped_performance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatex_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_grouped_latex_table(df, group_column, caption=\"Your Table Caption\", label=\"tab:your_label\"):\n",
    "    \"\"\"\n",
    "    Create a LaTeX table with grouped rows based on a specific column.\n",
    "\n",
    "    :param df: Pandas DataFrame to convert\n",
    "    :param group_column: Column name to group by\n",
    "    :param caption: Caption for the LaTeX table\n",
    "    :param label: Label for the LaTeX table\n",
    "    :return: String containing the LaTeX table code\n",
    "    \"\"\"\n",
    "    unique_groups = df[group_column].unique()\n",
    "    grouped = df.groupby(group_column)\n",
    "\n",
    "    latex_table = \"\\\\begin{table}[ht]\\n\\\\centering\\n\\\\begin{tabular}{|l|l|r|r|r|}\\n\\\\hline\\n\"\n",
    "    column_labels = \" & \".join(df.columns) + \" \\\\\\\\\\n\\\\hline\\n\"\n",
    "    latex_table += column_labels\n",
    "\n",
    "    for group in unique_groups:\n",
    "        group_df = grouped.get_group(group)\n",
    "        for i, row in group_df.iterrows():\n",
    "            if i == group_df.index[0]:  # First row of the group\n",
    "                latex_table += f\"\\\\multirow{{{len(group_df)}}}{{*}}{{{row[group_column]}}}\"\n",
    "            latex_table += \" & \" + \" & \".join([str(row[col]) for col in df.columns if col != group_column])\n",
    "            latex_table += \" \\\\\\\\\\n\"\n",
    "            if i == group_df.index[-1]:  # Last row of the group\n",
    "                latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    latex_table += \"\\\\end{tabular}\\n\"\n",
    "    latex_table += f\"\\\\caption{{{caption}}}\\n\"\n",
    "    latex_table += f\"\\\\label{{{label}}}\\n\"\n",
    "    latex_table += \"\\\\end{table}\"\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "# Example DataFrame\n",
    "# data = {\n",
    "#     \"Task Name\": [\"Task1\", \"Task1\", \"Task2\", \"Task2\", \"Task2\"],\n",
    "#     \"Modeling\": [\"Model A\", \"Model B\", \"Model A\", \"Model B\", \"Model C\"],\n",
    "#     \"RMSE\": [1.2, 1.3, 1.1, 1.4, 1.5],\n",
    "#     \"PCC\": [0.7, 0.75, 0.65, 0.8, 0.85],\n",
    "#     \"CCC\": [0.9, 0.85, 0.88, 0.86, 0.89]\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = create_grouped_latex_table(results_df, \"Task\", caption=\"Grouped Performance Metrics\", label=\"tab:grouped_performance\")\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6b6a79",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FeatureExtraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n           ^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2095786/3949228524.py\", line 22, in extract\n    features.extract_features(video=file, audio=file, caches=cache)\n  File \"/work/jingyiz4/mustard-demo/python_tools/features.py\", line 546, in extract_features\n    datum = run_openface(video, cache=caches[key], **kwargs[key])\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/mustard-demo/python_tools/caching.py\", line 190, in wrapper\n    result = function(*args, cache=cache, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/mustard-demo/python_tools/features.py\", line 411, in run_openface\n    returncode, _, error = generic.run(command, stdout=None)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/mustard-demo/python_tools/generic.py\", line 136, in run\n    with subprocess.Popen(\n         ^^^^^^^^^^^^^^^^^\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/subprocess.py\", line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"/work/jingyiz4/miniconda3/lib/python3.11/subprocess.py\", line 1950, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nFileNotFoundError: [Errno 2] No such file or directory: 'FeatureExtraction'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m     features\u001b[38;5;241m.\u001b[39mextract_features(video\u001b[38;5;241m=\u001b[39mfile, audio\u001b[38;5;241m=\u001b[39mfile, caches\u001b[38;5;241m=\u001b[39mcache)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mmap_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextract\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/projects/dataset_original/UMEME/media/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.mp*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow can I not\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIm quite sure that we will find some way or another\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly I joined her in the ceremony\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     58\u001b[0m     }\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/generic.py:56\u001b[0m, in \u001b[0;36mmap_parallel\u001b[0;34m(function, elements, workers, chunksize)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m workers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mworkers) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(function, elements))\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/multiprocessing/pool.py:48\u001b[0m, in \u001b[0;36mmapstar\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmapstar\u001b[39m(args):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;241m*\u001b[39margs))\n",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36mextract\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopensmile_eGeMAPSv02\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopensmile_eGeMAPSv02/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopensmile_vad_opensource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopensmile_vad_opensource/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m features\u001b[38;5;241m.\u001b[39mextract_features(video\u001b[38;5;241m=\u001b[39mfile, audio\u001b[38;5;241m=\u001b[39mfile, caches\u001b[38;5;241m=\u001b[39mcache)\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/features.py:546\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m()\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m key:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenface\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 546\u001b[0m         datum \u001b[38;5;241m=\u001b[39m run_openface(video, cache\u001b[38;5;241m=\u001b[39mcaches[key], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[key])\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovarep\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    548\u001b[0m         datum \u001b[38;5;241m=\u001b[39m run_covarep(audio, cache\u001b[38;5;241m=\u001b[39mcaches[key], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[key])\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/caching.py:190\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# run/save\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m result \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39margs, cache\u001b[38;5;241m=\u001b[39mcache, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    192\u001b[0m     write_hdfs(cache, result)\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/features.py:411\u001b[0m, in \u001b[0;36mrun_openface\u001b[0;34m()\u001b[0m\n\u001b[1;32m    405\u001b[0m command \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureExtraction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -out_dir \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# run OpenFace\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m returncode, _, error \u001b[38;5;241m=\u001b[39m generic\u001b[38;5;241m.\u001b[39mrun(command, stdout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    412\u001b[0m detail_file \u001b[38;5;241m=\u001b[39m output_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_of_details.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m detail_file\u001b[38;5;241m.\u001b[39munlink(missing_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/work/jingyiz4/mustard-demo/python_tools/generic.py:136\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stderr \u001b[38;5;241m==\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPIPE:\n\u001b[1;32m    134\u001b[0m     stderr \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mTemporaryFile(buffering\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m    137\u001b[0m     command,\n\u001b[1;32m    138\u001b[0m     stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[1;32m    139\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr,\n\u001b[1;32m    140\u001b[0m     stdin\u001b[38;5;241m=\u001b[39mstdin,\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    144\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/work/jingyiz4/miniconda3/lib/python3.11/subprocess.py:1950\u001b[0m, in \u001b[0;36m_execute_child\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FeatureExtraction'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from python_tools import caching, features\n",
    "from python_tools.generic import map_parallel\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/projects/dataset_processed/TPOT/twoertwein\")\n",
    "from python_tools.extract import extract_liwc, load_liwc\n",
    "\n",
    "def extract(file: Path) -> None:\n",
    "    name = file.with_suffix(\".hdf\").name\n",
    "    # openface+opensmile\n",
    "    cache = {}\n",
    "    if file.suffix == \".mp4\":\n",
    "        cache[\"openface\"] = Path(f\"openface/{name}\")\n",
    "    if file.suffix == \".mp3\" or \"Only\" not in file.name:\n",
    "        cache[\"opensmile_eGeMAPSv02\"] = Path(f\"opensmile_eGeMAPSv02/{name}\")\n",
    "        cache[\"opensmile_vad_opensource\"] = Path(f\"opensmile_vad_opensource/{name}\")\n",
    "    features.extract_features(video=file, audio=file, caches=cache)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    map_parallel(\n",
    "        extract,\n",
    "        Path(\"/projects/dataset_original/UMEME/media/\").glob(\"*.mp*\"),\n",
    "        workers=7,\n",
    "    )\n",
    "\n",
    "    sentences = (\n",
    "        \"How can I not\",\n",
    "        \"Im quite sure that we will find some way or another\",\n",
    "        \"Ella Jorgenson made the pudding\",\n",
    "        \"The floor was completely covered\",\n",
    "        \"They are just going to go ahead regardless\",\n",
    "        \"It has all been scheduled since Wednesday\",\n",
    "        \"I am going shopping\",\n",
    "        \"A preliminary study shows rats to be more inquisitive than once thought\",\n",
    "        \"Thats it the meeting is finished\",\n",
    "        \"I dont know how she could miss this opportunity\",\n",
    "        \"It is raining outside\",\n",
    "        \"Your dog is insane\",\n",
    "        \"She told me what you did\",\n",
    "        \"Your grandmother is on the phone\",\n",
    "        \"Only I joined her in the ceremony\",\n",
    "    )\n",
    "    data = {\n",
    "        \"file\": [],\n",
    "        \"name\": [],\n",
    "        \"sentence\": [],\n",
    "        \"valence\": [],\n",
    "        \"arousal\": [],\n",
    "        \"dominance\": [],\n",
    "        \"audio\": [],\n",
    "        \"video\": [],\n",
    "    }\n",
    "    for block in (\n",
    "        Path(\"/projects/dataset_original/UMEME/evaluation.txt\")\n",
    "        .read_text()\n",
    "        .split(\"\\n\\n\")\n",
    "    ):\n",
    "        if not block:\n",
    "            continue\n",
    "        name, *details = block.split(\"\\n\")\n",
    "        details = {\n",
    "            detail.split(\":\", 1)[0].strip(): detail.split(\":\", 1)[1].strip()\n",
    "            for detail in details\n",
    "        }\n",
    "\n",
    "        data[\"name\"].append(details[\"speaker\"])\n",
    "        match details[\"modality\"]:\n",
    "            case \"av\":\n",
    "                file = f\"{name}_original.mp4\"\n",
    "            case \"video\":\n",
    "                file = f\"{name}_videoOnly.mp4\"\n",
    "            case \"audio\":\n",
    "                file = f\"{name}_audioOnly.mp3\"\n",
    "            case _:\n",
    "                assert False, _\n",
    "        assert (Path(\"/projects/dataset_original/UMEME/media\") / file).is_file()\n",
    "        data[\"file\"].append(file)\n",
    "        data[\"sentence\"].append(\n",
    "            sentences[int(name.split(\"-\", 1)[0].split(\"S\", 1)[1][:-1]) - 1]\n",
    "        )\n",
    "        data[\"arousal\"].append(float(details[\"act\"].split(\"+\", 1)[0]))\n",
    "        data[\"valence\"].append(float(details[\"val\"].split(\"+\", 1)[0]))\n",
    "        data[\"dominance\"].append(float(details[\"dom\"].split(\"+\", 1)[0]))\n",
    "        data[\"audio\"].append(details.get(\"audio\", \"\"))\n",
    "        data[\"video\"].append(details.get(\"video\", \"\"))\n",
    "\n",
    "    # copy labels of uni-modal ratings but discard their features\n",
    "    data = pd.DataFrame(data)\n",
    "    unimodal = data.loc[data[\"file\"].apply(lambda x: \"Only\" in x)].copy()\n",
    "    unimodal[\"file\"] = unimodal[\"file\"].apply(lambda x: x.split(\".\")[0])\n",
    "    labels = (\"arousal\", \"valence\", \"dominance\")\n",
    "    for key in (\"audio\", \"video\"):\n",
    "        for label in labels:\n",
    "            data[f\"meta_{label}_{key}\"] = float(\"NaN\")\n",
    "    for index, row in data.iterrows():\n",
    "        if \"Only\" in row[\"file\"]:\n",
    "            continue\n",
    "        for key in (\"audio\", \"video\"):\n",
    "            match = unimodal[\"file\"] == f\"{row[key]}_{key}Only\"\n",
    "            if not match.any():\n",
    "                print(f\"{row[key]}_{key}Only\")\n",
    "                continue\n",
    "            assert match.sum() == 1\n",
    "            unimodal_index = match[match].index[0]\n",
    "\n",
    "            for label in labels:\n",
    "                data.loc[index, f\"meta_{label}_{key}\"] = unimodal.loc[\n",
    "                    unimodal_index, label\n",
    "                ]\n",
    "    data = data.loc[data[\"file\"].apply(lambda x: \"Only\" not in x)]\n",
    "\n",
    "    # model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "    # embeddings = pd.DataFrame(\n",
    "    #     model.encode(data[\"sentence\"].str.lower().tolist()), index=data.index\n",
    "    # )\n",
    "\n",
    "    liwc = load_liwc()\n",
    "    liwc_features = extract_liwc(liwc, data[\"sentence\"])\n",
    "\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            data,\n",
    "            # embeddings.add_prefix(\"all_minilm_l12_v2_\"),\n",
    "            liwc_features.add_prefix(\"liwc_\"),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    display(data.head())\n",
    "    \n",
    "    # caching.write_hdfs(Path(\"all_minilm_l12_v2.hdf\"), {\"df\": data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cde0cb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudpickle in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: dask in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (2023.12.1)\n",
      "Requirement already satisfied: distributed in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (2023.12.1)\n",
      "Requirement already satisfied: pympi-ling in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (1.70.2)\n",
      "Requirement already satisfied: nltk in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Collecting beartype\n",
      "  Downloading beartype-0.16.4-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: click>=8.1 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from dask) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from dask) (23.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from dask) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from dask) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from dask) (7.0.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from distributed) (3.1.2)\n",
      "Requirement already satisfied: locket>=1.0.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from distributed) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from distributed) (1.0.7)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from distributed) (5.9.5)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from distributed) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from distributed) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from distributed) (6.3.3)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from distributed) (1.26.18)\n",
      "Requirement already satisfied: zict>=3.0.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from distributed) (3.0.0)\n",
      "Requirement already satisfied: joblib in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from jinja2>=2.10.3->distributed) (2.1.1)\n",
      "Downloading beartype-0.16.4-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m819.1/819.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: beartype\n",
      "Successfully installed beartype-0.16.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cloudpickle dask distributed pympi-ling nltk beartype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the HDF file is \"all_minilm_l12_v2.hdf\" and the key for the dataframe is \"df\"\n",
    "file_path = \"all_minilm_l12_v2.hdf\"\n",
    "\n",
    "# Read the dataframe from the HDF file\n",
    "try:\n",
    "    stored_df = pd.read_hdf(file_path, key='df')\n",
    "except Exception as e:\n",
    "    stored_df = f\"An error occurred while reading the HDF file: {e}\"\n",
    "\n",
    "stored_df\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muvi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
