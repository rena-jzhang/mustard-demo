{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vSTHaWeYdnY"
      },
      "outputs": [],
      "source": [
        "pip install transformers torch torchvision jsonlines scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (repodata.json): | ^C\n",
            "\\ \n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "conda env create -f environment.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk6uzO-jOH7g",
        "outputId": "3804c97f-114f-46d0-de6a-7b94e28482d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'MUStARD/'\n",
            "/Users/jingyi/CMU/23s/research/MUStARD\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/soujanyaporia/MUStARD\n",
        "%cd MUStARD/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgeJ4CZHOIjD"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!mkdir -p data/features\n",
        "!gdown -O data/features --id --folder 1Ff1WDObGKqpfbvy7-H1mD8YWvBS-Kf26\n",
        "!gdown --id 1GYv74vN80iX_IkEmkJhkjDRGxLvraWuZ\n",
        "!unzip BERT_text_features.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dXCJ8e1hX6oO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 1721\n",
            "(552, 2048)\n",
            "(552, 283)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pytorch_model.bin:  59%|█████▉    | 1.56G/2.63G [10:04<06:53, 2.59MB/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m config \u001b[39m=\u001b[39m CONFIG_BY_KEY[\u001b[39m\"\u001b[39m\u001b[39mtav\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m data \u001b[39m=\u001b[39m DataLoader(config)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m train(config, data)\n",
            "\u001b[1;32m/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m feature_modes \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mprecomputed\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mprecomputed\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(LM_VERSION, use_fast\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m model \u001b[39m=\u001b[39m TextFeatureOPTModel(LM_VERSION, \u001b[39mlist\u001b[39;49m(train_features\u001b[39m.\u001b[39;49mkeys()), tokenizer, feature_modes\u001b[39m=\u001b[39;49mfeature_modes)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
            "\u001b[1;32m/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, opt_model_name, feature_types, tokenizer, feature_modes):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39msuper\u001b[39m(TextFeatureOPTModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(opt_model_name)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types \u001b[39m=\u001b[39m feature_types\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jingyi/CMU/23s/research/MUStARD/mustard.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_modes \u001b[39m=\u001b[39m feature_modes  \u001b[39m# mode for each feature type: 'raw' or 'precomputed'\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/transformers/modeling_utils.py:3057\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3055\u001b[0m         \u001b[39m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[1;32m   3056\u001b[0m         filename \u001b[39m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[0;32m-> 3057\u001b[0m         resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   3058\u001b[0m             pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs\n\u001b[1;32m   3059\u001b[0m         )\n\u001b[1;32m   3060\u001b[0m \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[1;32m   3061\u001b[0m     \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   3063\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3064\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[1;32m   3065\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs,\n\u001b[1;32m   3066\u001b[0m     )\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/transformers/utils/hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    428\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    431\u001b[0m         path_or_repo_id,\n\u001b[1;32m    432\u001b[0m         filename,\n\u001b[1;32m    433\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    434\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    435\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    436\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    437\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    438\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    439\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    440\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    441\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    442\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/huggingface_hub/file_download.py:1461\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1459\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1461\u001b[0m     http_get(\n\u001b[1;32m   1462\u001b[0m         url_to_download,\n\u001b[1;32m   1463\u001b[0m         temp_file,\n\u001b[1;32m   1464\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1465\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1466\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1467\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1468\u001b[0m     )\n\u001b[1;32m   1470\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1471\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, _nb_retries)\u001b[0m\n\u001b[1;32m    539\u001b[0m new_resume_size \u001b[39m=\u001b[39m resume_size\n\u001b[1;32m    540\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    542\u001b[0m         \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m             progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/urllib3/response.py:934\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 934\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    936\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    937\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/urllib3/response.py:877\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    875\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 877\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    879\u001b[0m flush_decoder \u001b[39m=\u001b[39m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data)\n\u001b[1;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/urllib3/response.py:812\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    809\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    811\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 812\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    814\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages/urllib3/response.py:797\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    795\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torchvision.models import resnet50\n",
        "from collections import defaultdict\n",
        "from config import CONFIG_BY_KEY, Config\n",
        "from data_loader import DataLoader, DataHelper\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "LM_VERSION = 'facebook/opt-1.3b'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class TextFeatureOPTModel(nn.Module):\n",
        "    def __init__(self, opt_model_name, feature_types, tokenizer, feature_modes):\n",
        "        super(TextFeatureOPTModel, self).__init__()\n",
        "        self.opt_model = AutoModelForCausalLM.from_pretrained(opt_model_name).to(device)\n",
        "        self.feature_types = feature_types\n",
        "        self.feature_modes = feature_modes \n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.modules = defaultdict(nn.ModuleDict)\n",
        "\n",
        "        # Initialize modules for different feature types\n",
        "        for feature_type in feature_types:\n",
        "            if feature_type == 'video' and feature_modes.get(feature_type) == 'raw':\n",
        "                self.modules[feature_type]['encoder'] = resnet50(pretrained=True).to(device)\n",
        "                self.modules[feature_type]['encoder'].fc = nn.Identity()\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(2048, self.opt_model.config.hidden_size).to(device)\n",
        "        \n",
        "            elif feature_type == 'audio' and feature_modes.get(feature_type) == 'raw':\n",
        "                self.modules[feature_type]['encoder'] = nn.Sequential(\n",
        "                    nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool1d(kernel_size=2),\n",
        "                ).to(device)\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(32, self.opt_model.config.hidden_size).to(device)\n",
        "\n",
        "    def forward(self, text_input_ids, features):\n",
        "        input_embeddings = self.opt_model.get_input_embeddings()\n",
        "        text_embeddings = input_embeddings(text_input_ids)\n",
        "\n",
        "        feature_inputs = []\n",
        "\n",
        "        # Process non-text features\n",
        "        for i, feature_type in enumerate(self.feature_types):\n",
        "            mode = self.feature_modes.get(feature_type)\n",
        "\n",
        "            if mode == 'raw':\n",
        "                encoder = self.modules[feature_type]['encoder']\n",
        "                embedding_transform = self.modules[feature_type]['embedding_transform']\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    feature_input = encoder(features[i])\n",
        "                feature_input = torch.flatten(feature_input, start_dim=1)\n",
        "\n",
        "                feature_embeddings = embedding_transform(feature_input)\n",
        "                feature_inputs.append(feature_embeddings.unsqueeze(1))\n",
        "            \n",
        "            elif mode == 'precomputed':\n",
        "                # Directly use the precomputed features\n",
        "                feature_embeddings = embedding_transform(features[i])\n",
        "                feature_inputs.append(features[i].unsqueeze(1))\n",
        "\n",
        "        # Concatenate feature embeddings with text embeddings\n",
        "        combined_embeddings = [text_embeddings] + feature_inputs\n",
        "        combined_embeddings = torch.cat(combined_embeddings, dim=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.opt_model.generate(inputs_embeds=combined_embeddings)\n",
        "\n",
        "        decoded_texts = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return decoded_texts\n",
        "    \n",
        "def evaluate_model(model, test_features, test_output, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_output)):\n",
        "            text_input_ids = torch.tensor(test_features['text'][i], dtype=torch.long).to(device)\n",
        "            non_text_feature_inputs = []\n",
        "\n",
        "            # Check and process each type of feature\n",
        "            for feature_type in list(test_features.keys())[1:]:\n",
        "                if test_features[feature_type]:\n",
        "                    non_text_feature_inputs.append(torch.tensor(test_features[feature_type][i]).to(device))\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            labels = torch.tensor(test_output[i], dtype=torch.long).to(device)\n",
        "\n",
        "            outputs = model(text_input_ids, non_text_feature_inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "    average_loss = total_loss / len(test_output)\n",
        "    accuracy = np.mean(np.array(predictions) == np.array(actuals))\n",
        "    print(f'Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(actuals, predictions))\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(actuals, predictions))\n",
        "\n",
        "    return average_loss, accuracy\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, filename):\n",
        "    # Create directory if it does not exist\n",
        "    # os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path):\n",
        "    \n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "\n",
        "    model.train()\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(len(train_output)):\n",
        "            text_input_ids = torch.tensor(train_features['text'][i], dtype=torch.long).to(device)\n",
        "            non_text_feature_inputs = []\n",
        "\n",
        "            for feature_type in list(train_features.keys())[1:]:\n",
        "                if train_features[feature_type]:\n",
        "                    non_text_feature_inputs.append(torch.tensor(train_features[feature_type][i]).to(device))\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            labels = torch.tensor(train_output[i], dtype=torch.long).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text_input_ids, non_text_feature_inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        average_loss = total_loss / len(train_output)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint if it's the best model so far\n",
        "        if average_loss < best_loss:\n",
        "            best_loss = average_loss\n",
        "            checkpoint_filename = os.path.join(checkpoint_path, f'model_checkpoint_epoch_{epoch+1}.pth')\n",
        "            save_checkpoint(model, optimizer, epoch, checkpoint_filename)\n",
        "\n",
        "def train_io(config, data, train_index, test_index):\n",
        "    train_input, train_output = data.get_split(train_index)\n",
        "    test_input, test_output = data.get_split(test_index)\n",
        "\n",
        "    datahelper = DataHelper(train_input, train_output, test_input, test_output, config, data)\n",
        "\n",
        "    train_features = {'text': [], 'video': [], 'audio': []}\n",
        "    test_features = {'text': [], 'video': [], 'audio': []}\n",
        "\n",
        "    if config.use_target_text:\n",
        "        if config.use_bert:\n",
        "            train_features['text'] = datahelper.get_target_bert_feature(mode=\"train\")\n",
        "            test_features['text'] = datahelper.get_target_bert_feature(mode=\"test\")\n",
        "        else:\n",
        "            train_features['text'] = datahelper.vectorize_utterance(mode=\"train\")\n",
        "            test_features['text'] = datahelper.vectorize_utterance(mode=\"test\")\n",
        "\n",
        "    if config.use_target_video:\n",
        "        train_features['video'] = datahelper.get_target_video_pool(mode=\"train\")\n",
        "        test_features['video'] = datahelper.get_target_video_pool(mode=\"test\")\n",
        "        \n",
        "    if config.use_target_audio:\n",
        "        train_features['audio'] = datahelper.get_target_audio_pool(mode=\"train\")\n",
        "        test_features['audio'] = datahelper.get_target_audio_pool(mode=\"test\")\n",
        "\n",
        "    # print(train_features['video'].shape)\n",
        "    # print(train_features['audio'].shape)\n",
        "\n",
        "\n",
        "    # Check if any modality is being used\n",
        "    if all(len(features) == 0 for features in train_features.values()):\n",
        "        raise ValueError(\"Invalid modalities\")\n",
        "\n",
        "    return train_features, train_output, test_features, test_output\n",
        "\n",
        "\n",
        "def train(config, data):\n",
        "    all_indices = data.get_all_indices_shuffled()\n",
        "\n",
        "    split_point = int(len(all_indices) * 0.8)  # Example: 80% for training, 20% for testing\n",
        "    train_index = all_indices[:split_point]\n",
        "    test_index = all_indices[split_point:]\n",
        "\n",
        "    train_features, train_output, test_features, test_output = train_io(config=config, data=data, train_index=train_index, test_index=test_index)\n",
        "    non_text_feature_modes = {'video': 'precomputed', 'audio': 'precomputed'}\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LM_VERSION, use_fast=False)\n",
        "    model = TextFeatureOPTModel(LM_VERSION, list(non_text_feature_modes.keys()), tokenizer, feature_modes=non_text_feature_modes).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    num_epochs = 1\n",
        "    \n",
        "    \n",
        "    train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path = 'checkpoints/')\n",
        "    average_loss, accuracy = evaluate_model(model, test_features, test_output, criterion, device)\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = CONFIG_BY_KEY[\"tav\"]\n",
        "    data = DataLoader(config)\n",
        "    train(config, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting h5py\n",
            "  Downloading h5py-3.10.0-cp38-cp38-macosx_11_0_arm64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonlines in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (4.0.0)\n",
            "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (3.8.1)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (1.24.4)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from jsonlines) (23.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from nltk) (1.3.2)\n",
            "Installing collected packages: h5py\n",
            "Successfully installed h5py-3.10.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install h5py jsonlines nltk numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_iXThAuWnql"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     text_input = [\"A beautiful sunset over the mountains.\", \"Delicious food at a local restaurant.\"]\n",
        "#     sample_images = torch.rand(2, 3, 224, 224).to(device)\n",
        "\n",
        "#     # sample_audio = torch.rand(2, 1, audio_length).to(device)  # Move audio tensors to the specified device\n",
        "#     feature_data = {\n",
        "#         'video': sample_images,\n",
        "#         # 'audio': sample_audio\n",
        "#     }\n",
        "#     feature_types = list(feature_data.keys())\n",
        "#     features = list(feature_data.values())\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(LM_VERSION, use_fast=False)\n",
        "\n",
        "#     # Tokenize the text input\n",
        "#     text_input_ids = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "\n",
        "#     # Create an instance of TextFeatureOPTModel\n",
        "#     text_feature_opt_model = TextFeatureOPTModel(LM_VERSION, feature_types, tokenizer).to(device)\n",
        "\n",
        "#     # Perform inference with both image and audio features\n",
        "#     outputs = text_feature_opt_model(text_input_ids, features)\n",
        "#     print(outputs)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6Vr2eb0kvWu",
        "outputId": "a94e475e-a8d5-4a9f-e16f-841032e405d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MUStARD\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7DHNJmWkwFa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
