{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-vSTHaWeYdnY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.21.3)\n",
            "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (1.11.0)\n",
            "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.12.0)\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines) (22.2.0)\n",
            "Collecting joblib>=1.1.1\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.5.0\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.0.1)\n",
            "Installing collected packages: threadpoolctl, scipy, jsonlines, joblib, scikit-learn\n",
            "Successfully installed joblib-1.3.2 jsonlines-4.0.0 scikit-learn-1.3.2 scipy-1.11.4 threadpoolctl-3.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch torchvision jsonlines scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving notices: ...working... done\n",
            "Collecting package metadata (repodata.json): - WARNING conda.models.version:get_matcher(544): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0\n",
            "WARNING conda.models.version:get_matcher(544): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.6.0.*, but conda is ignoring the .* and treating it as 1.6.0\n",
            "WARNING conda.models.version:get_matcher(544): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0\n",
            "WARNING conda.models.version:get_matcher(544): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\n",
            "done\n",
            "Solving environment: - \n",
            "Found conflicts! Looking for incompatible packages.\n",
            "This can take several minutes.  Press CTRL-C to abort.\n",
            "                                                                                 -   -failed\n",
            "Solving environment: / \n",
            "Found conflicts! Looking for incompatible packages.\n",
            "This can take several minutes.  Press CTRL-C to abort.\n",
            "Examining h5py:  84%|██████████████████████▋    | 16/19 [01:41<00:28,  9.39s/i/ ^C\n",
            "                                                                               failed\n",
            "\n",
            "CondaError: KeyboardInterrupt\n",
            "\n",
            "\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "conda env create -f environment.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk6uzO-jOH7g",
        "outputId": "3804c97f-114f-46d0-de6a-7b94e28482d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'MUStARD/'\n",
            "/Users/jingyi/CMU/23s/research/MUStARD\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/soujanyaporia/MUStARD\n",
        "# %cd MUStARD/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgeJ4CZHOIjD"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!mkdir -p data/features\n",
        "!gdown -O data/features --id --folder 1Ff1WDObGKqpfbvy7-H1mD8YWvBS-Kf26\n",
        "!gdown --id 1GYv74vN80iX_IkEmkJhkjDRGxLvraWuZ\n",
        "!unzip BERT_text_features.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dXCJ8e1hX6oO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 1746\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 394.00 MiB (GPU 0; 14.75 GiB total capacity; 13.80 GiB already allocated; 3.06 MiB free; 13.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/ec2-user/mustard-demo/mustard.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=250'>251</a>\u001b[0m config \u001b[39m=\u001b[39m CONFIG_BY_KEY[\u001b[39m\"\u001b[39m\u001b[39mtav\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=251'>252</a>\u001b[0m data \u001b[39m=\u001b[39m DataPreper(config)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=252'>253</a>\u001b[0m train(config, data)\n",
            "\u001b[1;32m/home/ec2-user/mustard-demo/mustard.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=237'>238</a>\u001b[0m non_text_feature_modes \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mprecomputed\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mprecomputed\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=239'>240</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(LM_VERSION, use_fast\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=240'>241</a>\u001b[0m model \u001b[39m=\u001b[39m TextFeatureOPTModel(LM_VERSION, \u001b[39mlist\u001b[39;49m(non_text_feature_modes\u001b[39m.\u001b[39;49mkeys()), tokenizer, feature_modes\u001b[39m=\u001b[39;49mnon_text_feature_modes)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=241'>242</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=242'>243</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
            "\u001b[1;32m/home/ec2-user/mustard-demo/mustard.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, opt_model_name, feature_types, tokenizer, feature_modes):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39msuper\u001b[39m(TextFeatureOPTModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(opt_model_name)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types \u001b[39m=\u001b[39m feature_types\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-52-54-129-18.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_modes \u001b[39m=\u001b[39m feature_modes \n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    905\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 907\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 601\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    602\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    603\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    903\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    904\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 905\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 394.00 MiB (GPU 0; 14.75 GiB total capacity; 13.80 GiB already allocated; 3.06 MiB free; 13.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torchvision.models import resnet50\n",
        "from collections import defaultdict\n",
        "from config import CONFIG_BY_KEY, Config\n",
        "from data_loader import DataPreper, DataHelper\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.text_features = features['text']\n",
        "        self.video_features = features['video']\n",
        "        self.audio_features = features['audio']\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_features[idx]\n",
        "        video = self.video_features[idx]\n",
        "        audio = self.audio_features[idx]\n",
        "        label = self.labels[idx]\n",
        "        return text, video, audio, label\n",
        "\n",
        "LM_VERSION = 'facebook/opt-1.3b'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class TextFeatureOPTModel(nn.Module):\n",
        "    def __init__(self, opt_model_name, feature_types, tokenizer, feature_modes):\n",
        "        super(TextFeatureOPTModel, self).__init__()\n",
        "        self.opt_model = AutoModelForCausalLM.from_pretrained(opt_model_name).to(device)\n",
        "        self.feature_types = feature_types\n",
        "        self.feature_modes = feature_modes \n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.modules = defaultdict(nn.ModuleDict)\n",
        "\n",
        "        # Initialize modules for different feature types\n",
        "        for feature_type in feature_types:\n",
        "            if feature_type == 'video':\n",
        "                if feature_modes.get(feature_type) == 'raw':\n",
        "                    self.modules[feature_type]['encoder'] = resnet50(pretrained=True).to(device)\n",
        "                    self.modules[feature_type]['encoder'].fc = nn.Identity()\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(2048, self.opt_model.config.hidden_size).to(device)\n",
        "        \n",
        "            elif feature_type == 'audio':\n",
        "                if feature_modes.get(feature_type) == 'raw':\n",
        "                    self.modules[feature_type]['encoder'] = nn.Sequential(\n",
        "                        nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool1d(kernel_size=2),\n",
        "                    ).to(device)\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(32, self.opt_model.config.hidden_size).to(device)\n",
        "\n",
        "    def forward(self, text_input, features):\n",
        "        text_input_ids = self.tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "        input_embeddings = self.opt_model.get_input_embeddings()\n",
        "        text_embeddings = input_embeddings(text_input_ids)\n",
        "\n",
        "        feature_inputs = []\n",
        "\n",
        "        # Process non-text features\n",
        "        for i, feature_type in enumerate(self.feature_types):\n",
        "            mode = self.feature_modes.get(feature_type)\n",
        "\n",
        "            if mode == 'raw':\n",
        "                encoder = self.modules[feature_type]['encoder']\n",
        "                embedding_transform = self.modules[feature_type]['embedding_transform']\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    feature_input = encoder(features[i])\n",
        "                feature_input = torch.flatten(feature_input, start_dim=1)\n",
        "\n",
        "                feature_embeddings = embedding_transform(feature_input)\n",
        "                feature_inputs.append(feature_embeddings.unsqueeze(1))\n",
        "            \n",
        "            elif mode == 'precomputed':\n",
        "                # Directly use the precomputed features\n",
        "                feature_embeddings = embedding_transform(features[i])\n",
        "                feature_inputs.append(features[i].unsqueeze(1))\n",
        "\n",
        "        # Concatenate feature embeddings with text embeddings\n",
        "        combined_embeddings = [text_embeddings] + feature_inputs\n",
        "        combined_embeddings = torch.cat(combined_embeddings, dim=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.opt_model.generate(inputs_embeds=combined_embeddings)\n",
        "\n",
        "        decoded_texts = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return decoded_texts\n",
        "    \n",
        "def evaluate_model(model, test_features, test_output, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_output)):\n",
        "            text_input = test_features['text'][i]\n",
        "            non_text_feature_inputs = []\n",
        "\n",
        "            # Check and process each type of feature\n",
        "            for feature_type in list(test_features.keys())[1:]:\n",
        "                if test_features[feature_type]:\n",
        "                    non_text_feature_inputs.append(torch.tensor(test_features[feature_type][i]).to(device))\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            labels = torch.tensor(test_output[i], dtype=torch.long).to(device)\n",
        "\n",
        "            outputs = model(text_input, non_text_feature_inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "            \n",
        "\n",
        "    average_loss = total_loss / len(test_output)\n",
        "    accuracy = np.mean(np.array(predictions) == np.array(actuals))\n",
        "    print(f'Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(actuals, predictions))\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(actuals, predictions))\n",
        "\n",
        "    return average_loss, accuracy\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, filename):\n",
        "    # Create directory if it does not exist\n",
        "    # os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path):\n",
        "    \n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "\n",
        "    model.train()\n",
        "    best_loss = float('inf')\n",
        "    \n",
        "    # train_dataset = CustomDataset(train_features, train_output)\n",
        "    # train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        \n",
        "        for i in range(len(train_output)):\n",
        "            text_input = train_features['text'][i]\n",
        "            non_text_feature_inputs = []\n",
        "\n",
        "            for feature_type in list(train_features.keys())[1:]:\n",
        "                non_text_feature_inputs.append(torch.tensor(train_features[feature_type][i]).to(device))\n",
        "                \n",
        "            labels = torch.tensor(train_output[i], dtype=torch.long).to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text_input, non_text_feature_inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            del text_input, non_text_feature_inputs, labels\n",
        "\n",
        "        average_loss = total_loss / len(train_output)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint if it's the best model so far\n",
        "        if average_loss < best_loss:\n",
        "            best_loss = average_loss\n",
        "            checkpoint_filename = os.path.join(checkpoint_path, f'model_checkpoint_epoch_{epoch+1}.pth')\n",
        "            save_checkpoint(model, optimizer, epoch, checkpoint_filename)\n",
        "\n",
        "def train_io(config, data, train_index, test_index):\n",
        "    train_input, train_output = data.get_split(train_index)\n",
        "    test_input, test_output = data.get_split(test_index)\n",
        "\n",
        "    datahelper = DataHelper(train_input, train_output, test_input, test_output, config, data)\n",
        "\n",
        "    train_features = {'text': [], 'video': [], 'audio': []}\n",
        "    test_features = {'text': [], 'video': [], 'audio': []}\n",
        "\n",
        "    if config.use_target_text:\n",
        "        if config.use_bert:\n",
        "            train_features['text'] = datahelper.get_target_bert_feature(mode=\"train\")\n",
        "            test_features['text'] = datahelper.get_target_bert_feature(mode=\"test\")\n",
        "        else:\n",
        "            train_features['text'] = datahelper.vectorize_utterance(mode=\"train\")\n",
        "            test_features['text'] = datahelper.vectorize_utterance(mode=\"test\")\n",
        "\n",
        "    if config.use_target_video:\n",
        "        train_features['video'] = datahelper.get_target_video_pool(mode=\"train\")\n",
        "        test_features['video'] = datahelper.get_target_video_pool(mode=\"test\")\n",
        "        \n",
        "    if config.use_target_audio:\n",
        "        train_features['audio'] = datahelper.get_target_audio_pool(mode=\"train\")\n",
        "        test_features['audio'] = datahelper.get_target_audio_pool(mode=\"test\")\n",
        "\n",
        "    # print(train_features['video'].shape)\n",
        "    # print(train_features['audio'].shape)\n",
        "\n",
        "\n",
        "    # Check if any modality is being used\n",
        "    if all(len(features) == 0 for features in train_features.values()):\n",
        "        raise ValueError(\"Invalid modalities\")\n",
        "\n",
        "    return train_features, train_output, test_features, test_output\n",
        "\n",
        "\n",
        "def train(config, data):\n",
        "    all_indices = data.get_all_indices_shuffled()\n",
        "\n",
        "    split_point = int(len(all_indices) * 0.8)  # Example: 80% for training, 20% for testing\n",
        "    train_index = all_indices[:split_point]\n",
        "    test_index = all_indices[split_point:]\n",
        "\n",
        "    train_features, train_output, test_features, test_output = train_io(config=config, data=data, train_index=train_index, test_index=test_index)\n",
        "    non_text_feature_modes = {'video': 'precomputed', 'audio': 'precomputed'}\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LM_VERSION, use_fast=False)\n",
        "    model = TextFeatureOPTModel(LM_VERSION, list(non_text_feature_modes.keys()), tokenizer, feature_modes=non_text_feature_modes).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    num_epochs = 1\n",
        "    \n",
        "    train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path = 'checkpoints/')\n",
        "    average_loss, accuracy = evaluate_model(model, test_features, test_output, criterion, device)\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = CONFIG_BY_KEY[\"tav\"]\n",
        "    data = DataPreper(config)\n",
        "    train(config, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting h5py\n",
            "  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonlines in /opt/conda/lib/python3.10/site-packages (4.0.0)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.24.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines) (22.2.0)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
            "Collecting click\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h5py, click, nltk\n",
            "Successfully installed click-8.1.7 h5py-3.10.0 nltk-3.8.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install h5py jsonlines nltk numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_iXThAuWnql"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     text_input = [\"A beautiful sunset over the mountains.\", \"Delicious food at a local restaurant.\"]\n",
        "#     sample_images = torch.rand(2, 3, 224, 224).to(device)\n",
        "\n",
        "#     # sample_audio = torch.rand(2, 1, audio_length).to(device)  # Move audio tensors to the specified device\n",
        "#     feature_data = {\n",
        "#         'video': sample_images,\n",
        "#         # 'audio': sample_audio\n",
        "#     }\n",
        "#     feature_types = list(feature_data.keys())\n",
        "#     features = list(feature_data.values())\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(LM_VERSION, use_fast=False)\n",
        "\n",
        "#     # Tokenize the text input\n",
        "#     text_input_ids = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "\n",
        "#     # Create an instance of TextFeatureOPTModel\n",
        "#     text_feature_opt_model = TextFeatureOPTModel(LM_VERSION, feature_types, tokenizer).to(device)\n",
        "\n",
        "#     # Perform inference with both image and audio features\n",
        "#     outputs = text_feature_opt_model(text_input_ids, features)\n",
        "#     print(outputs)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6Vr2eb0kvWu",
        "outputId": "a94e475e-a8d5-4a9f-e16f-841032e405d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MUStARD\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7DHNJmWkwFa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
