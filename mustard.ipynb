{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-vSTHaWeYdnY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.0.1)\n",
            "Requirement already satisfied: torchvision in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.15.2)\n",
            "Collecting jsonlines\n",
            "  Obtaining dependency information for jsonlines from https://files.pythonhosted.org/packages/f8/62/d9ba6323b9202dd2fe166beab8a86d29465c41a0288cbe229fac60c1ab8d/jsonlines-4.0.0-py3-none-any.whl.metadata\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: scikit-learn in /opt/conda/envs/pytorch/lib/python3.10/site-packages (1.3.1)\n",
            "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
            "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/8f/3e/4b8b40eb3c80aeaf360f0361d956d129bb3d23b2a3ecbe3a04a8f3bdd6d3/regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
            "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/eb/3d/eee5f3c572a3f4db2ebabf5bd4f284f356078a5b5d27e6229b4450d5c5e4/tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/35/8f/892d2e1bcfceb7ee3f9b055ac4bb111e31d25f0a38c7f44d1d59bf7a501a/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchvision) (10.0.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonlines) (23.1.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: safetensors, regex, jsonlines, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.19.4 jsonlines-4.0.0 regex-2023.10.3 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch torchvision jsonlines scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk6uzO-jOH7g",
        "outputId": "3804c97f-114f-46d0-de6a-7b94e28482d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'MUStARD/'\n",
            "/Users/jingyi/CMU/23s/research/MUStARD\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/soujanyaporia/MUStARD\n",
        "# %cd MUStARD/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DgeJ4CZHOIjD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!mkdir -p data/features\n",
        "!gdown -O data/features --id --folder 1Ff1WDObGKqpfbvy7-H1mD8YWvBS-Kf26\n",
        "!gdown --id 1GYv74vN80iX_IkEmkJhkjDRGxLvraWuZ\n",
        "!unzip BERT_text_features.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dXCJ8e1hX6oO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n",
            "Total GPU Memory: 15102.06 MB\n",
            "Currently Allocated Memory: 945.72 MB\n",
            "Currently Cached Memory: 968.00 MB\n",
            "Free Memory: 14156.34 MB\n",
            "Vocab size: 1700\n",
            "load tokenizer\n",
            "GPU: Tesla T4\n",
            "Total GPU Memory: 15102.06 MB\n",
            "Currently Allocated Memory: 945.72 MB\n",
            "Currently Cached Memory: 968.00 MB\n",
            "Free Memory: 14156.34 MB\n",
            "load model\n",
            "GPU: Tesla T4\n",
            "Total GPU Memory: 15102.06 MB\n",
            "Currently Allocated Memory: 1181.79 MB\n",
            "Currently Cached Memory: 1204.00 MB\n",
            "Free Memory: 13920.27 MB\n",
            "3\n",
            "NEW DP\n",
            "torch.Size([2048])\n",
            "torch.Size([283])\n",
            "torch.Size([1, 1, 2048])\n",
            "torch.Size([1, 1, 283])\n",
            "torch.Size([1, 38, 512])\n",
            "torch.Size([1, 1, 512])\n",
            "torch.Size([1, 1, 512])\n",
            "[\"-, in about seven months you're gonna have something that you're gonna love more than any guy you've ever been out with.\"]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not list",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/ec2-user/mustard-demo/mustard.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=316'>317</a>\u001b[0m \u001b[39m# print(\"before data\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=317'>318</a>\u001b[0m \u001b[39m# gpu_monitor()\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=318'>319</a>\u001b[0m data \u001b[39m=\u001b[39m DataPreper(config)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=319'>320</a>\u001b[0m train(config, data)\n",
            "\u001b[1;32m/home/ec2-user/mustard-demo/mustard.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=305'>306</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=306'>307</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=308'>309</a>\u001b[0m train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mcheckpoints/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=309'>310</a>\u001b[0m average_loss, accuracy \u001b[39m=\u001b[39m evaluate_model(model, test_features, test_output, criterion, device)\n",
            "\u001b[1;32m/home/ec2-user/mustard-demo/mustard.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=230'>231</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=231'>232</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(text_input, non_text_feature_inputs)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=232'>233</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=233'>234</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bec2-54-144-248-164.compute-1.amazonaws.com/home/ec2-user/mustard-demo/mustard.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=235'>236</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1163\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1164\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1165\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2995\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2996\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
            "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not list"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "from collections import defaultdict\n",
        "from config import CONFIG_BY_KEY\n",
        "from data_loader import DataPreper, DataHelper\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "\n",
        "def gpu_monitor():\n",
        "\n",
        "    # Check if CUDA is available\n",
        "    if torch.cuda.is_available():\n",
        "        # Get the ID of the current GPU\n",
        "        device_id = torch.cuda.current_device()\n",
        "\n",
        "        # Get the name of the current GPU\n",
        "        gpu_name = torch.cuda.get_device_name(device_id)\n",
        "\n",
        "        # Get the total memory of the current GPU\n",
        "        total_memory = torch.cuda.get_device_properties(device_id).total_memory\n",
        "\n",
        "        # Convert bytes to megabytes\n",
        "        total_memory_in_MB = total_memory / (1024**2)\n",
        "\n",
        "        # Get the current memory usage\n",
        "        current_memory_allocated = torch.cuda.memory_allocated(device_id)\n",
        "        current_memory_allocated_in_MB = current_memory_allocated / (1024**2)\n",
        "\n",
        "        # Get the current memory cached\n",
        "        current_memory_cached = torch.cuda.memory_reserved(device_id)\n",
        "        current_memory_cached_in_MB = current_memory_cached / (1024**2)\n",
        "\n",
        "        # Calculate free memory\n",
        "        free_memory_in_MB = total_memory_in_MB - current_memory_allocated_in_MB\n",
        "\n",
        "        print(f\"GPU: {gpu_name}\")\n",
        "        print(f\"Total GPU Memory: {total_memory_in_MB:.2f} MB\")\n",
        "        print(f\"Currently Allocated Memory: {current_memory_allocated_in_MB:.2f} MB\")\n",
        "        print(f\"Currently Cached Memory: {current_memory_cached_in_MB:.2f} MB\")\n",
        "        print(f\"Free Memory: {free_memory_in_MB:.2f} MB\")\n",
        "    else:\n",
        "        print(\"CUDA is not available. No GPU detected.\")\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.text_features = features['text']\n",
        "        self.video_features = features['video']\n",
        "        self.audio_features = features['audio']\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_features[idx]\n",
        "        video = self.video_features[idx]\n",
        "        audio = self.audio_features[idx]\n",
        "        label = self.labels[idx]\n",
        "        return text, video, audio, label\n",
        "\n",
        "LM_VERSION = 't5-small'\n",
        "# 'facebook/opt-2.7b'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class TextFeatureOPTModel(nn.Module):\n",
        "    def __init__(self, model_name, feature_types, tokenizer, feature_modes):\n",
        "        super(TextFeatureOPTModel, self).__init__()\n",
        "        # self.opt_model = AutoModelForCausalLM.from_pretrained(opt_model_name).to(device)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "        self.feature_types = feature_types\n",
        "        self.feature_modes = feature_modes \n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.modules = defaultdict(nn.ModuleDict)\n",
        "\n",
        "        # Initialize modules for different feature types\n",
        "        for feature_type in feature_types:\n",
        "            if feature_type == 'video':\n",
        "                if feature_modes.get(feature_type) == 'raw':\n",
        "                    self.modules[feature_type]['encoder'] = resnet50(pretrained=True).to(device)\n",
        "                    self.modules[feature_type]['encoder'].fc = nn.Identity()\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(2048, self.model.config.hidden_size).to(device)\n",
        "        \n",
        "            elif feature_type == 'audio':\n",
        "                if feature_modes.get(feature_type) == 'raw':\n",
        "                    self.modules[feature_type]['encoder'] = nn.Sequential(\n",
        "                        nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool1d(kernel_size=2),\n",
        "                    ).to(device)\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(283, self.model.config.hidden_size).double().to(device)\n",
        "\n",
        "    def forward(self, text_input, non_text_features):\n",
        "        print(\"NEW DP\")\n",
        "        text_input_ids = self.tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "        input_embeddings = self.model.get_input_embeddings()\n",
        "        text_embeddings = input_embeddings(text_input_ids)\n",
        "\n",
        "        feature_inputs = []\n",
        "        for i in non_text_features:\n",
        "            print(i.shape)\n",
        "\n",
        "        # Process non-text features\n",
        "        for i, feature_type in enumerate(self.feature_types):\n",
        "            mode = self.feature_modes.get(feature_type)\n",
        "\n",
        "            embedding_transform = self.modules[feature_type]['embedding_transform']\n",
        "            \n",
        "            if mode == 'raw':\n",
        "                encoder = self.modules[feature_type]['encoder']\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    feature_input = encoder(non_text_features[i])\n",
        "                feature_input = torch.flatten(feature_input, start_dim=1)\n",
        "\n",
        "                feature_embeddings = embedding_transform(feature_input)\n",
        "                feature_inputs.append(feature_embeddings.unsqueeze(1))\n",
        "            \n",
        "            elif mode == 'precomputed':\n",
        "                if non_text_features[i].dim() == 1:\n",
        "                    feature_input = non_text_features[i].unsqueeze(0).unsqueeze(0)\n",
        "                else:\n",
        "                    feature_input = non_text_features[i].unsqueeze(1)\n",
        "                    \n",
        "                print(feature_input.shape)\n",
        "                \n",
        "                # Directly use the precomputed features\n",
        "                feature_embeddings = embedding_transform(feature_input)\n",
        "                feature_inputs.append(feature_embeddings)\n",
        "\n",
        "        # Concatenate feature embeddings with text embeddings\n",
        "        \n",
        "        combined_embeddings = [text_embeddings] + feature_inputs\n",
        "        for i in combined_embeddings:\n",
        "            print(i.shape)\n",
        "        combined_embeddings = torch.cat(combined_embeddings, dim=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(inputs_embeds=combined_embeddings.float(), max_length = 50)\n",
        "\n",
        "        decoded_texts = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        print(decoded_texts)\n",
        "        return decoded_texts\n",
        "    \n",
        "def evaluate_model(model, test_features, test_output, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_output)):\n",
        "            text_input = test_features['text'][i]\n",
        "            non_text_feature_inputs = []\n",
        "\n",
        "            # Check and process each type of feature\n",
        "            for feature_type in list(test_features.keys())[1:]:\n",
        "                if test_features[feature_type]:\n",
        "                    non_text_feature_inputs.append(torch.tensor(test_features[feature_type][i]).to(device))\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            labels = torch.tensor(test_output[i], dtype=torch.long).to(device)\n",
        "\n",
        "            outputs = model(text_input, non_text_feature_inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "            \n",
        "\n",
        "    average_loss = total_loss / len(test_output)\n",
        "    accuracy = np.mean(np.array(predictions) == np.array(actuals))\n",
        "    print(f'Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(actuals, predictions))\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(actuals, predictions))\n",
        "\n",
        "    return average_loss, accuracy\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, filename):\n",
        "    # Create directory if it does not exist\n",
        "    # os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path):\n",
        "    \n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "\n",
        "    model.train()\n",
        "    best_loss = float('inf')\n",
        "    \n",
        "    # train_dataset = CustomDataset(train_features, train_output)\n",
        "    # train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(len(train_output)):\n",
        "            text_input = train_features['text'][i]\n",
        "            non_text_feature_inputs = []\n",
        "            print(len(train_features.keys()))\n",
        "            if len(train_features.keys()) > 1:\n",
        "                for feature_type in list(train_features.keys())[1:]:\n",
        "                    non_text_feature_inputs.append(torch.tensor(train_features[feature_type][i]).to(device))\n",
        "                    \n",
        "            labels = torch.tensor(train_output[i], dtype=torch.long).to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text_input, non_text_feature_inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            del text_input, non_text_feature_inputs, labels\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        average_loss = total_loss / len(train_output)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint if it's the best model so far\n",
        "        if average_loss < best_loss:\n",
        "            best_loss = average_loss\n",
        "            checkpoint_filename = os.path.join(checkpoint_path, f'model_checkpoint_epoch_{epoch+1}.pth')\n",
        "            save_checkpoint(model, optimizer, epoch, checkpoint_filename)\n",
        "\n",
        "\n",
        "def train_io(config, data, train_index, test_index):\n",
        "    train_input, train_output = data.get_split(train_index)\n",
        "    test_input, test_output = data.get_split(test_index)\n",
        "\n",
        "    datahelper = DataHelper(train_input, train_output, test_input, test_output, config, data)\n",
        "\n",
        "    train_features = {'text': [], 'video': [], 'audio': []}\n",
        "    test_features = {'text': [], 'video': [], 'audio': []}\n",
        "\n",
        "    if config.use_target_text:\n",
        "        if config.use_bert:\n",
        "            train_features['text'] = datahelper.get_target_bert_feature(mode=\"train\")\n",
        "            test_features['text'] = datahelper.get_target_bert_feature(mode=\"test\")\n",
        "        else:\n",
        "            train_features['text'] = datahelper.vectorize_utterance(mode=\"train\")\n",
        "            test_features['text'] = datahelper.vectorize_utterance(mode=\"test\")\n",
        "\n",
        "    if config.use_target_video:\n",
        "        train_features['video'] = datahelper.get_target_video_pool(mode=\"train\")\n",
        "        test_features['video'] = datahelper.get_target_video_pool(mode=\"test\")\n",
        "        \n",
        "    if config.use_target_audio:\n",
        "        train_features['audio'] = datahelper.get_target_audio_pool(mode=\"train\")\n",
        "        test_features['audio'] = datahelper.get_target_audio_pool(mode=\"test\")\n",
        "\n",
        "    # Check if any modality is being used\n",
        "    if all(len(features) == 0 for features in train_features.values()):\n",
        "        raise ValueError(\"Invalid modalities\")\n",
        "\n",
        "    return train_features, train_output, test_features, test_output\n",
        "\n",
        "\n",
        "def train(config, data):\n",
        "\n",
        "    \n",
        "    all_indices = data.get_all_indices_shuffled()\n",
        "\n",
        "    split_point = int(len(all_indices) * 0.8)  # Example: 80% for training, 20% for testing\n",
        "    train_index = all_indices[:split_point]\n",
        "    test_index = all_indices[split_point:]\n",
        "\n",
        "    gpu_monitor()\n",
        "\n",
        "    train_features, train_output, test_features, test_output = train_io(config=config, data=data, train_index=train_index, test_index=test_index)\n",
        "    non_text_feature_modes = {'video': 'precomputed', 'audio': 'precomputed'}\n",
        "\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(LM_VERSION, use_fast=False)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(LM_VERSION)\n",
        "    print(\"load tokenizer\")\n",
        "    gpu_monitor()\n",
        "\n",
        "    model = TextFeatureOPTModel(LM_VERSION, list(non_text_feature_modes.keys()), tokenizer, feature_modes=non_text_feature_modes).to(device)\n",
        "    print(\"load model\")\n",
        "    gpu_monitor()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    num_epochs = 1\n",
        "    \n",
        "    train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path = 'checkpoints/')\n",
        "    average_loss, accuracy = evaluate_model(model, test_features, test_output, criterion, device)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    config = CONFIG_BY_KEY[\"tav\"]\n",
        "    # print(\"before data\")\n",
        "    # gpu_monitor()\n",
        "    data = DataPreper(config)\n",
        "    train(config, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install h5py jsonlines nltk numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g_iXThAuWnql"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     text_input = [\"A beautiful sunset over the mountains.\", \"Delicious food at a local restaurant.\"]\n",
        "#     sample_images = torch.rand(2, 3, 224, 224).to(device)\n",
        "\n",
        "#     # sample_audio = torch.rand(2, 1, audio_length).to(device)  # Move audio tensors to the specified device\n",
        "#     feature_data = {\n",
        "#         'video': sample_images,\n",
        "#         # 'audio': sample_audio\n",
        "#     }\n",
        "#     feature_types = list(feature_data.keys())\n",
        "#     features = list(feature_data.values())\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(LM_VERSION, use_fast=False)\n",
        "\n",
        "#     # Tokenize the text input\n",
        "#     text_input_ids = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "\n",
        "#     # Create an instance of TextFeatureOPTModel\n",
        "#     text_feature_opt_model = TextFeatureOPTModel(LM_VERSION, feature_types, tokenizer).to(device)\n",
        "\n",
        "#     # Perform inference with both image and audio features\n",
        "#     outputs = text_feature_opt_model(text_input_ids, features)\n",
        "#     print(outputs)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6Vr2eb0kvWu",
        "outputId": "a94e475e-a8d5-4a9f-e16f-841032e405d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MUStARD\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7DHNJmWkwFa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
