{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vSTHaWeYdnY"
      },
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "552 138\n",
            "552 138\n",
            "552 138\n",
            "552 138\n",
            "552 138\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy\n",
        "# Replace 'your_file.p' with the path to your .p file\n",
        "file_path = '/work/jingyiz4/mustard-demo/data/split_indices.p'\n",
        "\n",
        "# Open the file in binary read mode\n",
        "with open(file_path, 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    data = pickle.load(file, encoding='latin1')\n",
        "\n",
        "    # Print the loaded data\n",
        "    # print(data)\n",
        "    for i, j in data:\n",
        "        print(len(i), len(j))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction\n",
            "sasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticastic    11\n",
            "sarcasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticastic       6\n",
            "'sarcasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticastic           2\n",
            "y,,,, mocking, like, \"hey, leonard, how was your dinfast with priya last night?\",y,,,, mocking,                                                                                                                                                  1\n",
            "taintainttainttainttaintittt w,t.s w w                                                                                                                                                                                                           1\n",
            "the a little love. well pretty much get down to that. well pretty much get down to that.                                                                                                                                                         1\n",
            "tttttsittt,,, leonard. yeah, good for you, leonard. yeah, good for you, leonard. yeah, good for you                                                                                                                                              1\n",
            "th theasticastic w wtastic w. i'm sorry, too, but there's no room for you in my wallet. i'm sorry,                                                                                                                                               1\n",
            "taining post is for you.taining post is for you.taining post is for you.taining post is for you.astict                                                                                                                                           1\n",
            "tain'ttttastictttttain't. we're not the only car                                                                                                                                                                                                 1\n",
            "the the theastictain the                                                                                                                                                                                                                         1\n",
            "h. it was an accident. it was an accident. it was an accident. it was an accident. it was a collision. it was a collision. it was a collision. it was a collision.                                                                               1\n",
            "t w the,, w w w w w w, i'm really looking forward to it.. i'm really looking forward to it.                                                                                                                                                      1\n",
            "tt. we're not a big celebrityt. w wttt w,. he's a minor celebrity. he's a minor celebrity                                                                                                                                                        1\n",
            "ttttt w wttt, which of these areas do you think is the most promising? or, what are you looking for?                                                                                                                                             1\n",
            "tain'ttain'ttain'ttain'ttain'tie ttt wtta                                                                                                                                                                                                        1\n",
            "tain'ttain'ttain'ttain'tastictain't wttt w                                                                                                                                                                                                       1\n",
            "taining the the the the - w. we're not the only one thetaastic                                                                                                                                                                                   1\n",
            "tain't. we're not a non-sceptictainasticastictain't wttt.                                                                                                                                                                                        1\n",
            "a new the theastic                                                                                                                                                                                                                               1\n",
            "tt theasticastictttt theasticastic wttt                                                                                                                                                                                                          1\n",
            "tain. we are not thetainingtain.th. we all agree on the w w w w                                                                                                                                                                                  1\n",
            "ttttt w wttttt.s w w w wtsttain                                                                                                                                                                                                                  1\n",
            "theed the,, ww-spreads w wived from the. i'll finish my coffee first.                                                                                                                                                                            1\n",
            "tainttainttainttaintain wttt w, i'm thirsty, so i'm thirsty                                                                                                                                                                                      1\n",
            "the theasticastic w wastic w wastic.  w w, we're not the                                                                                                                                                                                         1\n",
            "t a theasticastic w w w a w janice. janice. janice. janice. janice. janice. janice. janice. jan                                                                                                                                                  1\n",
            "ttttttsttt thant. she is sliced into three equal pieces. superman hits them and is immediately sliced into three equal pieces.                                                                                                                   1\n",
            "tain'tt. we're not thetaintain'tsttt.. we're not the only party!                                                                                                                                                                                 1\n",
            "t. we are not thettasticastic w w w w w a chicken suit? a chicken suit? a chicken suit? a chicken suit? a chicken suit?                                                                                                                          1\n",
            "tain'ttain'ttain'ttain't, wttt w, i'm not a big fan of                                                                                                                                                                                           1\n",
            "ttttt.sttt... bye-bye. bye-bye.                                                                                                                                                                                                                  1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.0000\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1 Score: 0.0000\n",
            "\n",
            "Prediction\n",
            "non-sarcastic                                                                                                                                                                                          86\n",
            "sarcastic                                                                                                                                                                                              28\n",
            "non-sarcasticastic                                                                                                                                                                                     10\n",
            "non-sarcarc                                                                                                                                                                                             2\n",
            "sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar                                                                                                                                         2\n",
            "non-sarcarc non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non             1\n",
            "non-sarcasticastic. non-sarcastic.                                                                                                                                                                      1\n",
            "not a non-profit    w wive  w????????????                                                                                                                                                               1\n",
            "non-sarcarcarc                                                                                                                                                                                          1\n",
            "non-sarca i'm the supply manager.                                                                                                                                                                       1\n",
            "sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar                                                                                 1\n",
            "sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar     1\n",
            "sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar                                                                                                         1\n",
            "non-sarcasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticasticastic                 1\n",
            "non-sarcastic.                                                                                                                                                                                          1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.4855\n",
            "Precision: 0.0792\n",
            "Recall: 0.0612\n",
            "F1 Score: 0.0665\n",
            "\n",
            "Prediction\n",
            "non-sarcastic                                                                                                                                                                                          100\n",
            "non-sarcasticastic                                                                                                                                                                                      22\n",
            "sarcastic                                                                                                                                                                                                7\n",
            "sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar      4\n",
            "non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non      2\n",
            "non-sarcastic......................                                                                                                                                                                      1\n",
            "non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non                                          1\n",
            "non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non              1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.4348\n",
            "Precision: 0.1580\n",
            "Recall: 0.0967\n",
            "F1 Score: 0.0956\n",
            "\n",
            "Prediction\n",
            "non-sarcastic                                                    67\n",
            "sarcastic                                                        67\n",
            "non-sarcastic......................                               1\n",
            "non-sarc my aunt marion gave them to me for my 12th birthday.     1\n",
            "non-sarccastic                                                    1\n",
            "sarcasticastic                                                    1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.5870\n",
            "Precision: 0.2015\n",
            "Recall: 0.1972\n",
            "F1 Score: 0.1980\n",
            "\n",
            "Prediction\n",
            "non-sarcastic                                                                                                                                                                                          106\n",
            "sarcastic                                                                                                                                                                                               26\n",
            "non-sarcasticastic                                                                                                                                                                                       4\n",
            "non-sarc wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow! wow                                                                               1\n",
            "sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar sar      1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6232\n",
            "Precision: 0.2726\n",
            "Recall: 0.2330\n",
            "F1 Score: 0.2346\n",
            "\n",
            "Prediction\n",
            "non-sarcastic         120\n",
            "sarcastic              14\n",
            "non-sarcasticastic      3\n",
            "sar sarcastic           1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6304\n",
            "Precision: 0.3705\n",
            "Recall: 0.2861\n",
            "F1 Score: 0.2708\n",
            "\n",
            "Prediction\n",
            "non-sarcastic         100\n",
            "sarcastic              36\n",
            "non-sarcasticastic      2\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6522\n",
            "Precision: 0.4422\n",
            "Recall: 0.4129\n",
            "F1 Score: 0.4147\n",
            "\n",
            "Prediction\n",
            "non-sarcastic                          73\n",
            "sarcastic                              63\n",
            "non-sarcastic......................     1\n",
            "non-sarcasticastic                      1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6304\n",
            "Precision: 0.3175\n",
            "Recall: 0.3145\n",
            "F1 Score: 0.3154\n",
            "\n",
            "Prediction\n",
            "non-sarcastic                          116\n",
            "sarcastic                               21\n",
            "non-sarcastic......................      1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6522\n",
            "Precision: 0.4666\n",
            "Recall: 0.4003\n",
            "F1 Score: 0.3867\n",
            "\n",
            "Prediction\n",
            "non-sarcastic                          94\n",
            "sarcastic                              43\n",
            "non-sarcastic......................     1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6667\n",
            "Precision: 0.4440\n",
            "Recall: 0.4276\n",
            "F1 Score: 0.4300\n",
            "\n",
            "Prediction\n",
            "non-sarcastic                          128\n",
            "sarcastic                                9\n",
            "non-sarcastic......................      1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6232\n",
            "Precision: 0.4994\n",
            "Recall: 0.3710\n",
            "F1 Score: 0.3296\n",
            "\n",
            "Prediction\n",
            "non-sarcastic                                                       84\n",
            "sarcastic                                                           53\n",
            "non-sarcastic sarcastic sarcastic sarcastic sarcastic sarcastic.     1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6957\n",
            "Precision: 0.4622\n",
            "Recall: 0.4553\n",
            "F1 Score: 0.4582\n",
            "\n",
            "Prediction\n",
            "non-sarcastic                                                              108\n",
            "sarcastic                                                                   29\n",
            "non-sarcastical.com.au. non-sarcastical.com.au. non-sarcastical.com.au.      1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6957\n",
            "Precision: 0.4897\n",
            "Recall: 0.4364\n",
            "F1 Score: 0.4351\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    83\n",
            "sarcastic        55\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6739\n",
            "Precision: 0.6645\n",
            "Recall: 0.6619\n",
            "F1 Score: 0.6628\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    89\n",
            "sarcastic        49\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6884\n",
            "Precision: 0.6805\n",
            "Recall: 0.6696\n",
            "F1 Score: 0.6718\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    99\n",
            "sarcastic        39\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.7029\n",
            "Precision: 0.7075\n",
            "Recall: 0.6726\n",
            "F1 Score: 0.6741\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    91\n",
            "sarcastic        47\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.7174\n",
            "Precision: 0.7137\n",
            "Recall: 0.6970\n",
            "F1 Score: 0.7003\n",
            "\n",
            "Prediction\n",
            "sarcastic        88\n",
            "non-sarcastic    50\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6812\n",
            "Precision: 0.7198\n",
            "Recall: 0.7084\n",
            "F1 Score: 0.6801\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    110\n",
            "sarcastic         28\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6957\n",
            "Precision: 0.7292\n",
            "Recall: 0.6522\n",
            "F1 Score: 0.6453\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    94\n",
            "sarcastic        44\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.7391\n",
            "Precision: 0.7420\n",
            "Recall: 0.7157\n",
            "F1 Score: 0.7201\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def postpros(res):\n",
        "    if res in ['sarcasm', 'sarcastic', '(sarcastic)']:\n",
        "        return 'sarcastic'\n",
        "    return res\n",
        "\n",
        "def analyze(df):\n",
        "    # Convert labels to lowercase if they are strings\n",
        "    actual_labels = df['Actual'].str.lower()\n",
        "    predicted_labels = df['Prediction'].str.lower()\n",
        "\n",
        "    # # Calculate and print the value counts of predicted labels\n",
        "    predicted_value_counts = predicted_labels.value_counts()\n",
        "    print(predicted_value_counts)\n",
        "        \n",
        "    predicted_labels = [postpros(res) for res in predicted_labels]\n",
        "\n",
        "    # Calculate the metrics\n",
        "    acc = accuracy_score(actual_labels, predicted_labels)\n",
        "    prec = precision_score(actual_labels, predicted_labels,  average='macro')\n",
        "    recall = recall_score(actual_labels, predicted_labels, average='macro')\n",
        "    f1 = f1_score(actual_labels, predicted_labels,  average='macro')\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print()\n",
        "\n",
        "num_epochs = 20\n",
        "for i in range(num_epochs):\n",
        "    folder_name = 't5-small_nopretrain_0.003_16_2024_0'\n",
        "    df = pd.read_csv(f\"/work/jingyiz4/mustard-demo/results/{folder_name}/predictions_actuals_{i}.csv\")\n",
        "    analyze(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (1.3.2)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from seaborn) (2.1.4)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.46.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.2/156.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Downloading Pillow-10.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.46.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-10.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
            "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.46.0 kiwisolver-1.4.5 matplotlib-3.8.2 pillow-10.1.0 pyparsing-3.1.1 seaborn-0.13.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn seaborn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DgeJ4CZHOIjD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/jingyiz4/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!mkdir -p data/features\n",
        "!gdown -O data/features --id --folder 1Ff1WDObGKqpfbvy7-H1mD8YWvBS-Kf26\n",
        "!gdown --id 1GYv74vN80iX_IkEmkJhkjDRGxLvraWuZ\n",
        "!unzip BERT_text_features.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dXCJ8e1hX6oO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before running\n",
            "CUDA is not available. No GPU detected.\n",
            "Vocab size: 1692\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Evaluating: 100%|██████████| 138/138 [00:32<00:00,  4.31batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/var/folders/p4/w2sdhfm935x2r11wpdd3zv600000gn/T/ipykernel_8312/1702567677.py:124: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  accuracy = np.mean(np.array(predictions) == np.array(actuals))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "from collections import defaultdict\n",
        "from config import CONFIG_BY_KEY\n",
        "from data_loader import DataPreper, DataHelper\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from utils import gpu_monitor, save_checkpoint, prompt_eng\n",
        "from tqdm import tqdm  # Import tqdm\n",
        "import csv\n",
        "\n",
        "\n",
        "LM_VERSION = 't5-small'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class TextFeatureOPTModel(nn.Module):\n",
        "    def __init__(self, model_name, feature_types, tokenizer, feature_modes):\n",
        "        super(TextFeatureOPTModel, self).__init__()\n",
        "        # self.opt_model = AutoModelForCausalLM.from_pretrained(opt_model_name).to(device)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "        self.feature_types = feature_types\n",
        "        self.feature_modes = feature_modes \n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.modules = defaultdict(nn.ModuleDict)\n",
        "\n",
        "        # Initialize modules for different feature types\n",
        "        for feature_type in feature_types:\n",
        "            if feature_type == 'video':\n",
        "                if feature_modes.get(feature_type) == 'raw':\n",
        "                    self.modules[feature_type]['encoder'] = resnet50(pretrained=True).to(device)\n",
        "                    self.modules[feature_type]['encoder'].fc = nn.Identity()\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(2048, self.model.config.hidden_size).to(device)\n",
        "        \n",
        "            elif feature_type == 'audio':\n",
        "                if feature_modes.get(feature_type) == 'raw':\n",
        "                    self.modules[feature_type]['encoder'] = nn.Sequential(\n",
        "                        nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool1d(kernel_size=2),\n",
        "                    ).to(device)\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(283, self.model.config.hidden_size).double().to(device)\n",
        "\n",
        "    def tokenize(self, text_input):\n",
        "        return self.tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "        \n",
        "    def forward(self, text_input_ids, non_text_features, label_ids = None):\n",
        "        self.model.eval()\n",
        "        input_embeddings = self.model.get_input_embeddings()\n",
        "        text_embeddings = input_embeddings(text_input_ids)\n",
        "\n",
        "        # Process non-text features\n",
        "        feature_inputs = []\n",
        "        for i, feature_type in enumerate(self.feature_types):\n",
        "            mode = self.feature_modes.get(feature_type)\n",
        "\n",
        "            embedding_transform = self.modules[feature_type]['embedding_transform']\n",
        "            \n",
        "            if mode == 'raw':\n",
        "                encoder = self.modules[feature_type]['encoder']\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    feature_input = encoder(non_text_features[i])\n",
        "                feature_input = torch.flatten(feature_input, start_dim=1)\n",
        "\n",
        "                feature_embeddings = embedding_transform(feature_input)\n",
        "                feature_inputs.append(feature_embeddings.unsqueeze(1))\n",
        "            \n",
        "            elif mode == 'precomputed':\n",
        "                if non_text_features[i].dim() == 1:\n",
        "                    feature_input = non_text_features[i].unsqueeze(0).unsqueeze(0)\n",
        "                else:\n",
        "                    feature_input = non_text_features[i].unsqueeze(1)\n",
        "                                    \n",
        "                # Directly use the precomputed features\n",
        "                feature_embeddings = embedding_transform(feature_input)\n",
        "                feature_inputs.append(feature_embeddings)\n",
        "\n",
        "        # Concatenate feature embeddings with text embeddings\n",
        "        combined_embeddings = [text_embeddings] + feature_inputs\n",
        "        combined_embeddings = torch.cat(combined_embeddings, dim=1)\n",
        "        \n",
        "        # print('combined feature shape' + str(combined_embeddings.shape))\n",
        "\n",
        "        # Handling both training and evaluation\n",
        "        if label_ids is not None:\n",
        "            # with torch.no_grad():\n",
        "            loss = self.model(inputs_embeds=combined_embeddings.float(), labels=label_ids, return_dict=True).loss\n",
        "                # print(f'output shape: {outputs.logits.shape}')\n",
        "            return loss\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(inputs_embeds=combined_embeddings.float(), max_length=50)\n",
        "            decoded_texts = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            return decoded_texts\n",
        "\n",
        "def evaluate_model(model, test_features, test_output, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Wrap the range function with tqdm for a progress bar\n",
        "        progress_bar = tqdm(range(len(test_output)), desc='Evaluating', unit='batch')\n",
        "\n",
        "        for i in progress_bar:\n",
        "            text_input_ids = model.tokenize(test_features['text'][i])\n",
        "\n",
        "            non_text_feature_inputs = []\n",
        "            for feature_type in list(test_features.keys())[1:]:\n",
        "                non_text_feature_inputs.append(torch.tensor(test_features[feature_type][i]).to(device))\n",
        "\n",
        "            predicted = model(text_input_ids, non_text_feature_inputs, label_ids=None)\n",
        "\n",
        "            predictions.extend(predicted)\n",
        "            actuals.extend(test_output)\n",
        "\n",
        "    accuracy = np.mean(np.array(predictions) == np.array(actuals))\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    \n",
        "    # Save predictions and actuals to a file\n",
        "    with open('predictions_actuals.csv', 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Prediction', 'Actual'])\n",
        "        for pred, act in zip(predictions, actuals):\n",
        "            writer.writerow([pred, act])\n",
        "\n",
        "    # # Confusion Matrix\n",
        "    # print(\"Confusion Matrix:\")\n",
        "    # print(confusion_matrix(actuals, predictions))\n",
        "\n",
        "    # # Classification Report\n",
        "    # print(\"Classification Report:\")\n",
        "    # print(classification_report(actuals, predictions))\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path):\n",
        "    \n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "\n",
        "    model.train()\n",
        "    best_loss = float('inf')\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        # Wrap the range function with tqdm for a progress bar\n",
        "        progress_bar = tqdm(range(len(train_output)), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
        "\n",
        "        for i in progress_bar:\n",
        "            text_input_ids = model.tokenize(train_features['text'][i])\n",
        "            label_ids = model.tokenize(train_output[i])\n",
        "\n",
        "            # Prepare non-text features\n",
        "            non_text_feature_inputs = []\n",
        "            if len(train_features.keys()) > 1:\n",
        "                for feature_type in list(train_features.keys())[1:]:\n",
        "                    non_text_feature_inputs.append(torch.tensor(train_features[feature_type][i]).to(device))\n",
        "                    \n",
        "            optimizer.zero_grad()\n",
        "            loss = model(text_input_ids, non_text_feature_inputs, label_ids)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            del text_input_ids, non_text_feature_inputs, label_ids\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'loss': total_loss / (i + 1)})\n",
        "\n",
        "        average_loss = total_loss / len(train_output)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint if it's the best model so far\n",
        "        if average_loss < best_loss:\n",
        "            best_loss = average_loss\n",
        "            checkpoint_filename = os.path.join(checkpoint_path, f'model_checkpoint_epoch_{epoch+1}.pth')\n",
        "            save_checkpoint(model, optimizer, epoch, checkpoint_filename)\n",
        "\n",
        "def train_io(config, data, train_index, test_index):\n",
        "    train_input, train_output = data.get_split(train_index)\n",
        "    test_input, test_output = data.get_split(test_index)\n",
        "\n",
        "    datahelper = DataHelper(train_input, train_output, test_input, test_output, config, data)\n",
        "\n",
        "    train_features = {}\n",
        "    test_features = {}\n",
        "\n",
        "    if config.use_target_text:\n",
        "        if config.use_bert:\n",
        "            train_features['text'] = datahelper.get_target_bert_feature(mode=\"train\")\n",
        "            test_features['text'] = datahelper.get_target_bert_feature(mode=\"test\")\n",
        "        else:\n",
        "            train_features['text'] = datahelper.vectorize_utterance(mode=\"train\")\n",
        "            test_features['text'] = datahelper.vectorize_utterance(mode=\"test\")\n",
        "            \n",
        "    if config.use_target_video:\n",
        "        train_features['video'] = datahelper.get_target_video_pool(mode=\"train\")\n",
        "        test_features['video'] = datahelper.get_target_video_pool(mode=\"test\")\n",
        "        \n",
        "    if config.use_target_audio:\n",
        "        train_features['audio'] = datahelper.get_target_audio_pool(mode=\"train\")\n",
        "        test_features['audio'] = datahelper.get_target_audio_pool(mode=\"test\")\n",
        "\n",
        "    # Check if any modality is being used\n",
        "    if all(len(features) == 0 for features in train_features.values()):\n",
        "        raise ValueError(\"Invalid modalities\")\n",
        "\n",
        "    return train_features, train_output, test_features, test_output\n",
        "    \n",
        "    \n",
        "def proprocess_output(train_output, test_output, class_mapping):\n",
        "    train_output = [class_mapping[i] for i in train_output]\n",
        "    test_output = [class_mapping[i] for i in test_output]\n",
        "    return train_output, test_output\n",
        "\n",
        "def train(config, data):\n",
        "    all_indices = data.get_all_indices_shuffled()\n",
        "\n",
        "    split_point = int(len(all_indices) * 0.8)  \n",
        "    train_index = all_indices[:split_point]\n",
        "    test_index = all_indices[split_point:]\n",
        "\n",
        "    # prepare data\n",
        "    train_features, train_output, test_features, test_output = train_io(config=config, data=data, train_index=train_index, test_index=test_index)\n",
        "    \n",
        "    sarcasm_mapping = {\n",
        "        0: \"Non-Sarcastic\",\n",
        "        1: \"Sarcastic\"\n",
        "    }\n",
        "    train_output, test_output = proprocess_output(train_output, test_output, class_mapping =  sarcasm_mapping)\n",
        "\n",
        "    template = \"Examine the input and categorize it as 'Sarcastic' or 'Non-Sarcastic' in the context of binary sarcasm detection: \"\n",
        "    train_features, test_features = prompt_eng(train_features, test_features, template)  # add the instructions and prompts\n",
        "    non_text_feature_modes = {'video': 'precomputed', 'audio': 'precomputed'}\n",
        "\n",
        "    # prepare model\n",
        "    tokenizer = T5Tokenizer.from_pretrained(LM_VERSION)\n",
        "    model = TextFeatureOPTModel(LM_VERSION, list(non_text_feature_modes.keys()), tokenizer, feature_modes=non_text_feature_modes).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    num_epochs = 3\n",
        "    # train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path = 'checkpoints/')\n",
        "    accuracy = evaluate_model(model, test_features, test_output, criterion, device)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    config = CONFIG_BY_KEY[\"tav\"]\n",
        "    \n",
        "    print(\"Before running\")\n",
        "    gpu_monitor()\n",
        "    \n",
        "    data = DataPreper(config)\n",
        "    train(config, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g_iXThAuWnql"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     text_input = [\"A beautiful sunset over the mountains.\", \"Delicious food at a local restaurant.\"]\n",
        "#     sample_images = torch.rand(2, 3, 224, 224).to(device)\n",
        "\n",
        "#     # sample_audio = torch.rand(2, 1, audio_length).to(device)  # Move audio tensors to the specified device\n",
        "#     feature_data = {\n",
        "#         'video': sample_images,\n",
        "#         # 'audio': sample_audio\n",
        "#     }\n",
        "#     feature_types = list(feature_data.keys())\n",
        "#     features = list(feature_data.values())\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(LM_VERSION, use_fast=False)\n",
        "\n",
        "#     # Tokenize the text input\n",
        "#     text_input_ids = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "\n",
        "#     # Create an instance of TextFeatureOPTModel\n",
        "#     text_feature_opt_model = TextFeatureOPTModel(LM_VERSION, feature_types, tokenizer).to(device)\n",
        "\n",
        "#     # Perform inference with both image and audio features\n",
        "#     outputs = text_feature_opt_model(text_input_ids, features)\n",
        "#     print(outputs)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7DHNJmWkwFa"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# from train.info import *\n",
        "\n",
        "class MMIDataset(Dataset):\n",
        "    # Multi-modal Individual Dataset\n",
        "    def __init__(self, data_type: str, dataset_name: str, dataset_rootdir: str = '../meta/',\n",
        "                 data_split=[0], nrows: int = -1, filter_dim_coordination=False, sample_frac: float = 1.0,\n",
        "                 slice_range: tuple = None):\n",
        "        eps = 1e-5\n",
        "        # assert dataset_name in ALL_DATASETS\n",
        "        # assert data_type in ALL_DATA_TYPES\n",
        "        self.dataset_name = dataset_name\n",
        "        print(f\"Loading Dataset {dataset_name},\", end=' ')\n",
        "\n",
        "        if dataset_name in ['vreed_av']:\n",
        "            dataset_rootdir = dataset_rootdir.replace('meta', 'meta_vreed')\n",
        "        elif dataset_name in ['iemocap_arousal', 'iemocap_valence']:\n",
        "            dataset_rootdir = dataset_rootdir.replace('meta', 'meta_iemocap')\n",
        "\n",
        "        df_list = [pd.read_csv(dataset_rootdir + dataset_name + f'_{idx}_{data_type}.csv')\n",
        "                   for idx in data_split] if nrows <= 0 else \\\n",
        "                  [pd.read_csv(dataset_rootdir + dataset_name + f'_{idx}_{data_type}.csv', nrows=nrows)\n",
        "                   for idx in data_split]\n",
        "\n",
        "        df = pd.concat(df_list, ignore_index=True)\n",
        "        df = df[slice_range[0]:slice_range[1]] if slice_range is not None else df\n",
        "        df = df.sample(frac=sample_frac, random_state=1706)\n",
        "\n",
        "        features = [ft for ft in df.columns if not (ft.startswith('meta') or ft == 'y')]\n",
        "        data = df[features]\n",
        "        data = data.dropna(axis='columns')\n",
        "        data = (data - data.mean()) / (data.std() + eps)\n",
        "\n",
        "        modalities = list(set([ft.split('_')[0] for ft in data.columns]))\n",
        "\n",
        "        self.data = {}\n",
        "        self.feature_size = {}\n",
        "        self.all_modalities = []\n",
        "        for modality in modalities:\n",
        "            feature_names = [ft for ft in data.columns if ft.startswith(modality)]\n",
        "            feature_df = data[feature_names]\n",
        "            data_mod = torch.tensor(feature_df.values).float()\n",
        "            # if filter_dim_coordination:\n",
        "            #     if len(data_mod[0]) != MODALITY_FEATURE_SIZE[modality]:\n",
        "            #         continue\n",
        "            self.data[modality] = data_mod\n",
        "            self.feature_size[modality] = len(self.data[modality][0])\n",
        "            self.all_modalities.append(modality)\n",
        "\n",
        "        label = torch.tensor(df[['y']].values)\n",
        "        # self.task_type = DATASET_TASK[dataset_name]\n",
        "        self.task_type = 'C'\n",
        "        if self.task_type == 'C':\n",
        "            self.label = F.one_hot(label, num_classes=4).float().squeeze(dim=-2)\n",
        "            self.label_std = None\n",
        "        else:\n",
        "            self.label = (label - label.mean()) / (label.std() + eps)\n",
        "            self.label_std = label.std().item()\n",
        "            print(f\"label std: {self.label_std},\", end=' ')\n",
        "        self.dataset_size = len(self.label)\n",
        "        print(f\"size: {self.dataset_size}\")\n",
        "        if self.task_type == 'C':\n",
        "            # self.class_num = DATASET_CLASS_NUMBER[dataset_name]\n",
        "            self.class_num = 4\n",
        "        else:\n",
        "            self.class_num = None\n",
        "        print(self.label)\n",
        "        print()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.label[idx]\n",
        "\n",
        "        feature = {}\n",
        "        for mod in self.all_modalities:\n",
        "            feature[mod] = self.data[mod][idx]\n",
        "\n",
        "        return feature, label, self.dataset_name, self.task_type\n",
        "\n",
        "\n",
        "def get_datasets(args) -> Dict[str, MMIDataset]:\n",
        "    # assert args.dataset_name in ALL_DATASETS\n",
        "    assert not args.multitask\n",
        "\n",
        "    return {data_type: MMIDataset(data_type, args.dataset_name, args.dataset_dir, args.dataset_split)\n",
        "            for data_type in ALL_DATA_TYPES}\n",
        "\n",
        "\n",
        "def get_multitask_datasets(args) -> Tuple[Dict[str, List[MMIDataset]], Dict[str, MMIDataset]]:\n",
        "    assert args.multitask\n",
        "    # assert all(dataset_name in ALL_DATASETS for dataset_name in args.dataset_name_list)\n",
        "\n",
        "    print(\"\\n[Loading validation datasets]\")\n",
        "    multitask_validation_datasets = {dataset_name: MMIDataset('validation', dataset_name, args.dataset_dir, args.dataset_split)\n",
        "                                     for dataset_name in args.dataset_name_list}\n",
        "\n",
        "    print(\"\\n[Loading training datasets]\")\n",
        "    multitask_training_datasets = {dataset_name: [] for dataset_name in args.dataset_name_list}\n",
        "\n",
        "    if args.balanced:\n",
        "        for dataset_name in args.dataset_name_list:\n",
        "            start_idx = 0\n",
        "            end_idx = args.per_dataset_size\n",
        "            for _ in range(args.max_num):\n",
        "                try:\n",
        "                    dataset = MMIDataset('training', dataset_name, args.dataset_dir, args.dataset_split,\n",
        "                                         slice_range=(start_idx, end_idx))\n",
        "                except:\n",
        "                    break\n",
        "                multitask_training_datasets[dataset_name].append(dataset)\n",
        "\n",
        "                start_idx = end_idx\n",
        "                end_idx += args.per_dataset_size\n",
        "\n",
        "    else:\n",
        "        NotImplementedError()\n",
        "\n",
        "    return multitask_training_datasets, multitask_validation_datasets\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
