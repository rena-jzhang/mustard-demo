{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-vSTHaWeYdnY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (4.35.2)\n",
            "Collecting SentencePiece\n",
            "  Downloading sentencepiece-0.1.99-cp38-cp38-macosx_11_0_arm64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (2.1.1)\n",
            "Requirement already satisfied: torchvision in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (0.16.1)\n",
            "Requirement already satisfied: jsonlines in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (4.0.0)\n",
            "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (1.3.2)\n",
            "Requirement already satisfied: h5py in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (3.10.0)\n",
            "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (3.8.1)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (1.24.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from torch) (2023.12.0)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from torchvision) (10.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from jsonlines) (23.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from requests->transformers) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniforge/base/envs/idealgpt/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: SentencePiece\n",
            "Successfully installed SentencePiece-0.1.99\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers SentencePiece torch torchvision jsonlines scikit-learn h5py jsonlines nltk numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk6uzO-jOH7g",
        "outputId": "3804c97f-114f-46d0-de6a-7b94e28482d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'MUStARD/'\n",
            "/Users/jingyi/CMU/23s/research/MUStARD\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/soujanyaporia/MUStARD\n",
        "# %cd MUStARD/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DgeJ4CZHOIjD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/jingyi/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!mkdir -p data/features\n",
        "!gdown -O data/features --id --folder 1Ff1WDObGKqpfbvy7-H1mD8YWvBS-Kf26\n",
        "!gdown --id 1GYv74vN80iX_IkEmkJhkjDRGxLvraWuZ\n",
        "!unzip BERT_text_features.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dXCJ8e1hX6oO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before running\n",
            "CUDA is not available. No GPU detected.\n",
            "Vocab size: 1692\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Evaluating: 100%|██████████| 138/138 [00:32<00:00,  4.31batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/var/folders/p4/w2sdhfm935x2r11wpdd3zv600000gn/T/ipykernel_8312/1702567677.py:124: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  accuracy = np.mean(np.array(predictions) == np.array(actuals))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "from collections import defaultdict\n",
        "from config import CONFIG_BY_KEY\n",
        "from data_loader import DataPreper, DataHelper\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from utils import gpu_monitor, save_checkpoint, prompt_eng\n",
        "from tqdm import tqdm  # Import tqdm\n",
        "import csv\n",
        "\n",
        "\n",
        "LM_VERSION = 't5-small'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class TextFeatureOPTModel(nn.Module):\n",
        "    def __init__(self, model_name, feature_types, tokenizer, feature_modes):\n",
        "        super(TextFeatureOPTModel, self).__init__()\n",
        "        # self.opt_model = AutoModelForCausalLM.from_pretrained(opt_model_name).to(device)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "        self.feature_types = feature_types\n",
        "        self.feature_modes = feature_modes \n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.modules = defaultdict(nn.ModuleDict)\n",
        "\n",
        "        # Initialize modules for different feature types\n",
        "        for feature_type in feature_types:\n",
        "            if feature_type == 'video':\n",
        "                if feature_modes.get(feature_type) == 'raw':\n",
        "                    self.modules[feature_type]['encoder'] = resnet50(pretrained=True).to(device)\n",
        "                    self.modules[feature_type]['encoder'].fc = nn.Identity()\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(2048, self.model.config.hidden_size).to(device)\n",
        "        \n",
        "            elif feature_type == 'audio':\n",
        "                if feature_modes.get(feature_type) == 'raw':\n",
        "                    self.modules[feature_type]['encoder'] = nn.Sequential(\n",
        "                        nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool1d(kernel_size=2),\n",
        "                    ).to(device)\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(283, self.model.config.hidden_size).double().to(device)\n",
        "\n",
        "    def tokenize(self, text_input):\n",
        "        return self.tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "        \n",
        "    def forward(self, text_input_ids, non_text_features, label_ids = None):\n",
        "        self.model.eval()\n",
        "        input_embeddings = self.model.get_input_embeddings()\n",
        "        text_embeddings = input_embeddings(text_input_ids)\n",
        "\n",
        "        # Process non-text features\n",
        "        feature_inputs = []\n",
        "        for i, feature_type in enumerate(self.feature_types):\n",
        "            mode = self.feature_modes.get(feature_type)\n",
        "\n",
        "            embedding_transform = self.modules[feature_type]['embedding_transform']\n",
        "            \n",
        "            if mode == 'raw':\n",
        "                encoder = self.modules[feature_type]['encoder']\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    feature_input = encoder(non_text_features[i])\n",
        "                feature_input = torch.flatten(feature_input, start_dim=1)\n",
        "\n",
        "                feature_embeddings = embedding_transform(feature_input)\n",
        "                feature_inputs.append(feature_embeddings.unsqueeze(1))\n",
        "            \n",
        "            elif mode == 'precomputed':\n",
        "                if non_text_features[i].dim() == 1:\n",
        "                    feature_input = non_text_features[i].unsqueeze(0).unsqueeze(0)\n",
        "                else:\n",
        "                    feature_input = non_text_features[i].unsqueeze(1)\n",
        "                                    \n",
        "                # Directly use the precomputed features\n",
        "                feature_embeddings = embedding_transform(feature_input)\n",
        "                feature_inputs.append(feature_embeddings)\n",
        "\n",
        "        # Concatenate feature embeddings with text embeddings\n",
        "        combined_embeddings = [text_embeddings] + feature_inputs\n",
        "        combined_embeddings = torch.cat(combined_embeddings, dim=1)\n",
        "        \n",
        "        # print('combined feature shape' + str(combined_embeddings.shape))\n",
        "\n",
        "        # Handling both training and evaluation\n",
        "        if label_ids is not None:\n",
        "            # with torch.no_grad():\n",
        "            loss = self.model(inputs_embeds=combined_embeddings.float(), labels=label_ids, return_dict=True).loss\n",
        "                # print(f'output shape: {outputs.logits.shape}')\n",
        "            return loss\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(inputs_embeds=combined_embeddings.float(), max_length=50)\n",
        "            decoded_texts = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            return decoded_texts\n",
        "\n",
        "def evaluate_model(model, test_features, test_output, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Wrap the range function with tqdm for a progress bar\n",
        "        progress_bar = tqdm(range(len(test_output)), desc='Evaluating', unit='batch')\n",
        "\n",
        "        for i in progress_bar:\n",
        "            text_input_ids = model.tokenize(test_features['text'][i])\n",
        "\n",
        "            non_text_feature_inputs = []\n",
        "            for feature_type in list(test_features.keys())[1:]:\n",
        "                non_text_feature_inputs.append(torch.tensor(test_features[feature_type][i]).to(device))\n",
        "\n",
        "            predicted = model(text_input_ids, non_text_feature_inputs, label_ids=None)\n",
        "\n",
        "            predictions.extend(predicted)\n",
        "            actuals.extend(test_output)\n",
        "\n",
        "    accuracy = np.mean(np.array(predictions) == np.array(actuals))\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    \n",
        "    \n",
        "    # Save predictions and actuals to a file\n",
        "    with open('predictions_actuals.csv', 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Prediction', 'Actual'])\n",
        "        for pred, act in zip(predictions, actuals):\n",
        "            writer.writerow([pred, act])\n",
        "\n",
        "    # # Confusion Matrix\n",
        "    # print(\"Confusion Matrix:\")\n",
        "    # print(confusion_matrix(actuals, predictions))\n",
        "\n",
        "    # # Classification Report\n",
        "    # print(\"Classification Report:\")\n",
        "    # print(classification_report(actuals, predictions))\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path):\n",
        "    \n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "\n",
        "    model.train()\n",
        "    best_loss = float('inf')\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        # Wrap the range function with tqdm for a progress bar\n",
        "        progress_bar = tqdm(range(len(train_output)), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
        "\n",
        "        for i in progress_bar:\n",
        "            text_input_ids = model.tokenize(train_features['text'][i])\n",
        "            label_ids = model.tokenize(train_output[i])\n",
        "\n",
        "            # Prepare non-text features\n",
        "            non_text_feature_inputs = []\n",
        "            if len(train_features.keys()) > 1:\n",
        "                for feature_type in list(train_features.keys())[1:]:\n",
        "                    non_text_feature_inputs.append(torch.tensor(train_features[feature_type][i]).to(device))\n",
        "                    \n",
        "            optimizer.zero_grad()\n",
        "            loss = model(text_input_ids, non_text_feature_inputs, label_ids)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            del text_input_ids, non_text_feature_inputs, label_ids\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'loss': total_loss / (i + 1)})\n",
        "\n",
        "        average_loss = total_loss / len(train_output)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint if it's the best model so far\n",
        "        if average_loss < best_loss:\n",
        "            best_loss = average_loss\n",
        "            checkpoint_filename = os.path.join(checkpoint_path, f'model_checkpoint_epoch_{epoch+1}.pth')\n",
        "            save_checkpoint(model, optimizer, epoch, checkpoint_filename)\n",
        "\n",
        "def train_io(config, data, train_index, test_index):\n",
        "    train_input, train_output = data.get_split(train_index)\n",
        "    test_input, test_output = data.get_split(test_index)\n",
        "\n",
        "    datahelper = DataHelper(train_input, train_output, test_input, test_output, config, data)\n",
        "\n",
        "    train_features = {}\n",
        "    test_features = {}\n",
        "\n",
        "    if config.use_target_text:\n",
        "        if config.use_bert:\n",
        "            train_features['text'] = datahelper.get_target_bert_feature(mode=\"train\")\n",
        "            test_features['text'] = datahelper.get_target_bert_feature(mode=\"test\")\n",
        "        else:\n",
        "            train_features['text'] = datahelper.vectorize_utterance(mode=\"train\")\n",
        "            test_features['text'] = datahelper.vectorize_utterance(mode=\"test\")\n",
        "            \n",
        "    if config.use_target_video:\n",
        "        train_features['video'] = datahelper.get_target_video_pool(mode=\"train\")\n",
        "        test_features['video'] = datahelper.get_target_video_pool(mode=\"test\")\n",
        "        \n",
        "    if config.use_target_audio:\n",
        "        train_features['audio'] = datahelper.get_target_audio_pool(mode=\"train\")\n",
        "        test_features['audio'] = datahelper.get_target_audio_pool(mode=\"test\")\n",
        "\n",
        "    # Check if any modality is being used\n",
        "    if all(len(features) == 0 for features in train_features.values()):\n",
        "        raise ValueError(\"Invalid modalities\")\n",
        "\n",
        "    return train_features, train_output, test_features, test_output\n",
        "    \n",
        "    \n",
        "def proprocess_output(train_output, test_output, class_mapping):\n",
        "    train_output = [class_mapping[i] for i in train_output]\n",
        "    test_output = [class_mapping[i] for i in test_output]\n",
        "    return train_output, test_output\n",
        "\n",
        "def train(config, data):\n",
        "    all_indices = data.get_all_indices_shuffled()\n",
        "\n",
        "    split_point = int(len(all_indices) * 0.8)  \n",
        "    train_index = all_indices[:split_point]\n",
        "    test_index = all_indices[split_point:]\n",
        "\n",
        "    # prepare data\n",
        "    train_features, train_output, test_features, test_output = train_io(config=config, data=data, train_index=train_index, test_index=test_index)\n",
        "    \n",
        "    sarcasm_mapping = {\n",
        "        0: \"Non-Sarcastic\",\n",
        "        1: \"Sarcastic\"\n",
        "    }\n",
        "    train_output, test_output = proprocess_output(train_output, test_output, class_mapping =  sarcasm_mapping)\n",
        "\n",
        "    template = \"Examine the input and categorize it as 'Sarcastic' or 'Non-Sarcastic' in the context of binary sarcasm detection: \"\n",
        "    train_features, test_features = prompt_eng(train_features, test_features, template)  # add the instructions and prompts\n",
        "    non_text_feature_modes = {'video': 'precomputed', 'audio': 'precomputed'}\n",
        "\n",
        "    # prepare model\n",
        "    tokenizer = T5Tokenizer.from_pretrained(LM_VERSION)\n",
        "    model = TextFeatureOPTModel(LM_VERSION, list(non_text_feature_modes.keys()), tokenizer, feature_modes=non_text_feature_modes).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    num_epochs = 3\n",
        "    # train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path = 'checkpoints/')\n",
        "    accuracy = evaluate_model(model, test_features, test_output, criterion, device)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    config = CONFIG_BY_KEY[\"tav\"]\n",
        "    \n",
        "    print(\"Before running\")\n",
        "    gpu_monitor()\n",
        "    \n",
        "    data = DataPreper(config)\n",
        "    train(config, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g_iXThAuWnql"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     text_input = [\"A beautiful sunset over the mountains.\", \"Delicious food at a local restaurant.\"]\n",
        "#     sample_images = torch.rand(2, 3, 224, 224).to(device)\n",
        "\n",
        "#     # sample_audio = torch.rand(2, 1, audio_length).to(device)  # Move audio tensors to the specified device\n",
        "#     feature_data = {\n",
        "#         'video': sample_images,\n",
        "#         # 'audio': sample_audio\n",
        "#     }\n",
        "#     feature_types = list(feature_data.keys())\n",
        "#     features = list(feature_data.values())\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(LM_VERSION, use_fast=False)\n",
        "\n",
        "#     # Tokenize the text input\n",
        "#     text_input_ids = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "\n",
        "#     # Create an instance of TextFeatureOPTModel\n",
        "#     text_feature_opt_model = TextFeatureOPTModel(LM_VERSION, feature_types, tokenizer).to(device)\n",
        "\n",
        "#     # Perform inference with both image and audio features\n",
        "#     outputs = text_feature_opt_model(text_input_ids, features)\n",
        "#     print(outputs)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6Vr2eb0kvWu",
        "outputId": "a94e475e-a8d5-4a9f-e16f-841032e405d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MUStARD\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7DHNJmWkwFa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
