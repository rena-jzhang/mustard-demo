{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vSTHaWeYdnY"
      },
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "552 138\n",
            "552 138\n",
            "552 138\n",
            "552 138\n",
            "552 138\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy\n",
        "# Replace 'your_file.p' with the path to your .p file\n",
        "file_path = '/work/jingyiz4/mustard-demo/data/split_indices.p'\n",
        "\n",
        "# Open the file in binary read mode\n",
        "with open(file_path, 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    data = pickle.load(file, encoding='latin1')\n",
        "\n",
        "    # Print the loaded data\n",
        "    # print(data)\n",
        "    for i, j in data:\n",
        "        print(len(i), len(j))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction\n",
            "non-sarcastic    138\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.4493\n",
            "Precision: 0.2246\n",
            "Recall: 0.5000\n",
            "F1 Score: 0.3100\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    138\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.4493\n",
            "Precision: 0.2246\n",
            "Recall: 0.5000\n",
            "F1 Score: 0.3100\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    138\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.4493\n",
            "Precision: 0.2246\n",
            "Recall: 0.5000\n",
            "F1 Score: 0.3100\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    138\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.4493\n",
            "Precision: 0.2246\n",
            "Recall: 0.5000\n",
            "F1 Score: 0.3100\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    138\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.4493\n",
            "Precision: 0.2246\n",
            "Recall: 0.5000\n",
            "F1 Score: 0.3100\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    138\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.4493\n",
            "Precision: 0.2246\n",
            "Recall: 0.5000\n",
            "F1 Score: 0.3100\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    136\n",
            "sarcastic          2\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.4638\n",
            "Precision: 0.7279\n",
            "Recall: 0.5132\n",
            "F1 Score: 0.3388\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    118\n",
            "sarcastic         20\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.5507\n",
            "Precision: 0.6750\n",
            "Recall: 0.5876\n",
            "F1 Score: 0.5049\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    98\n",
            "sarcastic        40\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.5942\n",
            "Precision: 0.6403\n",
            "Recall: 0.6167\n",
            "F1 Score: 0.5836\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    83\n",
            "sarcastic        55\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.5725\n",
            "Precision: 0.5863\n",
            "Recall: 0.5836\n",
            "F1 Score: 0.5714\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    77\n",
            "sarcastic        61\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6014\n",
            "Precision: 0.6088\n",
            "Recall: 0.6084\n",
            "F1 Score: 0.6014\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    89\n",
            "sarcastic        49\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6304\n",
            "Precision: 0.6584\n",
            "Recall: 0.6466\n",
            "F1 Score: 0.6271\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    69\n",
            "sarcastic        68\n",
            "sarcasm           1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6304\n",
            "Precision: 0.6304\n",
            "Recall: 0.6318\n",
            "F1 Score: 0.6295\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    69\n",
            "sarcastic        67\n",
            "sarcasm           1\n",
            "(sarcastic)       1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6594\n",
            "Precision: 0.6594\n",
            "Recall: 0.6611\n",
            "F1 Score: 0.6585\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    87\n",
            "sarcastic        49\n",
            "sarcasm           1\n",
            "(sarcastic)       1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6304\n",
            "Precision: 0.6542\n",
            "Recall: 0.6452\n",
            "F1 Score: 0.6281\n",
            "\n",
            "Prediction\n",
            "sarcastic        68\n",
            "non-sarcastic    68\n",
            "sarcasm           1\n",
            "(sarcastic)       1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6812\n",
            "Precision: 0.6805\n",
            "Recall: 0.6823\n",
            "F1 Score: 0.6801\n",
            "\n",
            "Prediction\n",
            "sarcastic        70\n",
            "non-sarcastic    67\n",
            "sarcasm           1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6739\n",
            "Precision: 0.6726\n",
            "Recall: 0.6742\n",
            "F1 Score: 0.6725\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    72\n",
            "sarcastic        64\n",
            "sarcasm           1\n",
            "(sarcastic)       1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6667\n",
            "Precision: 0.6692\n",
            "Recall: 0.6706\n",
            "F1 Score: 0.6664\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    75\n",
            "sarcastic        62\n",
            "sarcasm           1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6884\n",
            "Precision: 0.6943\n",
            "Recall: 0.6948\n",
            "F1 Score: 0.6884\n",
            "\n",
            "Prediction\n",
            "non-sarcastic    70\n",
            "sarcastic        67\n",
            "sarcasm           1\n",
            "Name: count, dtype: int64\n",
            "Accuracy: 0.6957\n",
            "Precision: 0.6964\n",
            "Recall: 0.6984\n",
            "F1 Score: 0.6951\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/work/jingyiz4/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# !pip install pandas\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "def postpros(res):\n",
        "    if res in ['sarcasm', 'sarcastic', '(sarcastic)']:\n",
        "        return 'sarcastic'\n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "def analyze(df):\n",
        "    # Convert labels to lowercase if they are strings\n",
        "    actual_labels = df['Actual'].str.lower()\n",
        "    predicted_labels = df['Prediction'].str.lower()\n",
        "\n",
        "    # # Calculate and print the value counts of predicted labels\n",
        "    predicted_value_counts = predicted_labels.value_counts()\n",
        "    print(predicted_value_counts)\n",
        "        \n",
        "    predicted_labels = [postpros(res) for res in predicted_labels]\n",
        "\n",
        "    # Calculate the metrics\n",
        "    acc = accuracy_score(actual_labels, predicted_labels)\n",
        "    prec = precision_score(actual_labels, predicted_labels,  average='macro')\n",
        "    recall = recall_score(actual_labels, predicted_labels, average='macro')\n",
        "    f1 = f1_score(actual_labels, predicted_labels,  average='macro')\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print()\n",
        "\n",
        "num_epochs = 20\n",
        "for i in range(num_epochs):\n",
        "    df = pd.read_csv(f\"/work/jingyiz4/mustard-demo/predictions_actuals_{i}.csv\")\n",
        "    analyze(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (1.3.2)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from seaborn) (2.1.4)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.46.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.2/156.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Downloading Pillow-10.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /work/jingyiz4/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.46.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-10.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
            "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.46.0 kiwisolver-1.4.5 matplotlib-3.8.2 pillow-10.1.0 pyparsing-3.1.1 seaborn-0.13.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn seaborn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DgeJ4CZHOIjD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/jingyiz4/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!mkdir -p data/features\n",
        "!gdown -O data/features --id --folder 1Ff1WDObGKqpfbvy7-H1mD8YWvBS-Kf26\n",
        "!gdown --id 1GYv74vN80iX_IkEmkJhkjDRGxLvraWuZ\n",
        "!unzip BERT_text_features.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dXCJ8e1hX6oO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before running\n",
            "CUDA is not available. No GPU detected.\n",
            "Vocab size: 1692\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Evaluating: 100%|██████████| 138/138 [00:32<00:00,  4.31batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/var/folders/p4/w2sdhfm935x2r11wpdd3zv600000gn/T/ipykernel_8312/1702567677.py:124: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  accuracy = np.mean(np.array(predictions) == np.array(actuals))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "from collections import defaultdict\n",
        "from config import CONFIG_BY_KEY\n",
        "from data_loader import DataPreper, DataHelper\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from utils import gpu_monitor, save_checkpoint, prompt_eng\n",
        "from tqdm import tqdm  # Import tqdm\n",
        "import csv\n",
        "\n",
        "\n",
        "LM_VERSION = 't5-small'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class TextFeatureOPTModel(nn.Module):\n",
        "    def __init__(self, model_name, feature_types, tokenizer, feature_modes):\n",
        "        super(TextFeatureOPTModel, self).__init__()\n",
        "        # self.opt_model = AutoModelForCausalLM.from_pretrained(opt_model_name).to(device)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "        self.feature_types = feature_types\n",
        "        self.feature_modes = feature_modes \n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.modules = defaultdict(nn.ModuleDict)\n",
        "\n",
        "        # Initialize modules for different feature types\n",
        "        for feature_type in feature_types:\n",
        "            if feature_type == 'video':\n",
        "                if feature_modes.get(feature_type) == 'raw':\n",
        "                    self.modules[feature_type]['encoder'] = resnet50(pretrained=True).to(device)\n",
        "                    self.modules[feature_type]['encoder'].fc = nn.Identity()\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(2048, self.model.config.hidden_size).to(device)\n",
        "        \n",
        "            elif feature_type == 'audio':\n",
        "                if feature_modes.get(feature_type) == 'raw':\n",
        "                    self.modules[feature_type]['encoder'] = nn.Sequential(\n",
        "                        nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool1d(kernel_size=2),\n",
        "                    ).to(device)\n",
        "                self.modules[feature_type]['embedding_transform'] = nn.Linear(283, self.model.config.hidden_size).double().to(device)\n",
        "\n",
        "    def tokenize(self, text_input):\n",
        "        return self.tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "        \n",
        "    def forward(self, text_input_ids, non_text_features, label_ids = None):\n",
        "        self.model.eval()\n",
        "        input_embeddings = self.model.get_input_embeddings()\n",
        "        text_embeddings = input_embeddings(text_input_ids)\n",
        "\n",
        "        # Process non-text features\n",
        "        feature_inputs = []\n",
        "        for i, feature_type in enumerate(self.feature_types):\n",
        "            mode = self.feature_modes.get(feature_type)\n",
        "\n",
        "            embedding_transform = self.modules[feature_type]['embedding_transform']\n",
        "            \n",
        "            if mode == 'raw':\n",
        "                encoder = self.modules[feature_type]['encoder']\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    feature_input = encoder(non_text_features[i])\n",
        "                feature_input = torch.flatten(feature_input, start_dim=1)\n",
        "\n",
        "                feature_embeddings = embedding_transform(feature_input)\n",
        "                feature_inputs.append(feature_embeddings.unsqueeze(1))\n",
        "            \n",
        "            elif mode == 'precomputed':\n",
        "                if non_text_features[i].dim() == 1:\n",
        "                    feature_input = non_text_features[i].unsqueeze(0).unsqueeze(0)\n",
        "                else:\n",
        "                    feature_input = non_text_features[i].unsqueeze(1)\n",
        "                                    \n",
        "                # Directly use the precomputed features\n",
        "                feature_embeddings = embedding_transform(feature_input)\n",
        "                feature_inputs.append(feature_embeddings)\n",
        "\n",
        "        # Concatenate feature embeddings with text embeddings\n",
        "        combined_embeddings = [text_embeddings] + feature_inputs\n",
        "        combined_embeddings = torch.cat(combined_embeddings, dim=1)\n",
        "        \n",
        "        # print('combined feature shape' + str(combined_embeddings.shape))\n",
        "\n",
        "        # Handling both training and evaluation\n",
        "        if label_ids is not None:\n",
        "            # with torch.no_grad():\n",
        "            loss = self.model(inputs_embeds=combined_embeddings.float(), labels=label_ids, return_dict=True).loss\n",
        "                # print(f'output shape: {outputs.logits.shape}')\n",
        "            return loss\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(inputs_embeds=combined_embeddings.float(), max_length=50)\n",
        "            decoded_texts = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            return decoded_texts\n",
        "\n",
        "def evaluate_model(model, test_features, test_output, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Wrap the range function with tqdm for a progress bar\n",
        "        progress_bar = tqdm(range(len(test_output)), desc='Evaluating', unit='batch')\n",
        "\n",
        "        for i in progress_bar:\n",
        "            text_input_ids = model.tokenize(test_features['text'][i])\n",
        "\n",
        "            non_text_feature_inputs = []\n",
        "            for feature_type in list(test_features.keys())[1:]:\n",
        "                non_text_feature_inputs.append(torch.tensor(test_features[feature_type][i]).to(device))\n",
        "\n",
        "            predicted = model(text_input_ids, non_text_feature_inputs, label_ids=None)\n",
        "\n",
        "            predictions.extend(predicted)\n",
        "            actuals.extend(test_output)\n",
        "\n",
        "    accuracy = np.mean(np.array(predictions) == np.array(actuals))\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    \n",
        "    # Save predictions and actuals to a file\n",
        "    with open('predictions_actuals.csv', 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Prediction', 'Actual'])\n",
        "        for pred, act in zip(predictions, actuals):\n",
        "            writer.writerow([pred, act])\n",
        "\n",
        "    # # Confusion Matrix\n",
        "    # print(\"Confusion Matrix:\")\n",
        "    # print(confusion_matrix(actuals, predictions))\n",
        "\n",
        "    # # Classification Report\n",
        "    # print(\"Classification Report:\")\n",
        "    # print(classification_report(actuals, predictions))\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path):\n",
        "    \n",
        "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "\n",
        "    model.train()\n",
        "    best_loss = float('inf')\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        # Wrap the range function with tqdm for a progress bar\n",
        "        progress_bar = tqdm(range(len(train_output)), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
        "\n",
        "        for i in progress_bar:\n",
        "            text_input_ids = model.tokenize(train_features['text'][i])\n",
        "            label_ids = model.tokenize(train_output[i])\n",
        "\n",
        "            # Prepare non-text features\n",
        "            non_text_feature_inputs = []\n",
        "            if len(train_features.keys()) > 1:\n",
        "                for feature_type in list(train_features.keys())[1:]:\n",
        "                    non_text_feature_inputs.append(torch.tensor(train_features[feature_type][i]).to(device))\n",
        "                    \n",
        "            optimizer.zero_grad()\n",
        "            loss = model(text_input_ids, non_text_feature_inputs, label_ids)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            del text_input_ids, non_text_feature_inputs, label_ids\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'loss': total_loss / (i + 1)})\n",
        "\n",
        "        average_loss = total_loss / len(train_output)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {average_loss:.4f}')\n",
        "\n",
        "        # Save checkpoint if it's the best model so far\n",
        "        if average_loss < best_loss:\n",
        "            best_loss = average_loss\n",
        "            checkpoint_filename = os.path.join(checkpoint_path, f'model_checkpoint_epoch_{epoch+1}.pth')\n",
        "            save_checkpoint(model, optimizer, epoch, checkpoint_filename)\n",
        "\n",
        "def train_io(config, data, train_index, test_index):\n",
        "    train_input, train_output = data.get_split(train_index)\n",
        "    test_input, test_output = data.get_split(test_index)\n",
        "\n",
        "    datahelper = DataHelper(train_input, train_output, test_input, test_output, config, data)\n",
        "\n",
        "    train_features = {}\n",
        "    test_features = {}\n",
        "\n",
        "    if config.use_target_text:\n",
        "        if config.use_bert:\n",
        "            train_features['text'] = datahelper.get_target_bert_feature(mode=\"train\")\n",
        "            test_features['text'] = datahelper.get_target_bert_feature(mode=\"test\")\n",
        "        else:\n",
        "            train_features['text'] = datahelper.vectorize_utterance(mode=\"train\")\n",
        "            test_features['text'] = datahelper.vectorize_utterance(mode=\"test\")\n",
        "            \n",
        "    if config.use_target_video:\n",
        "        train_features['video'] = datahelper.get_target_video_pool(mode=\"train\")\n",
        "        test_features['video'] = datahelper.get_target_video_pool(mode=\"test\")\n",
        "        \n",
        "    if config.use_target_audio:\n",
        "        train_features['audio'] = datahelper.get_target_audio_pool(mode=\"train\")\n",
        "        test_features['audio'] = datahelper.get_target_audio_pool(mode=\"test\")\n",
        "\n",
        "    # Check if any modality is being used\n",
        "    if all(len(features) == 0 for features in train_features.values()):\n",
        "        raise ValueError(\"Invalid modalities\")\n",
        "\n",
        "    return train_features, train_output, test_features, test_output\n",
        "    \n",
        "    \n",
        "def proprocess_output(train_output, test_output, class_mapping):\n",
        "    train_output = [class_mapping[i] for i in train_output]\n",
        "    test_output = [class_mapping[i] for i in test_output]\n",
        "    return train_output, test_output\n",
        "\n",
        "def train(config, data):\n",
        "    all_indices = data.get_all_indices_shuffled()\n",
        "\n",
        "    split_point = int(len(all_indices) * 0.8)  \n",
        "    train_index = all_indices[:split_point]\n",
        "    test_index = all_indices[split_point:]\n",
        "\n",
        "    # prepare data\n",
        "    train_features, train_output, test_features, test_output = train_io(config=config, data=data, train_index=train_index, test_index=test_index)\n",
        "    \n",
        "    sarcasm_mapping = {\n",
        "        0: \"Non-Sarcastic\",\n",
        "        1: \"Sarcastic\"\n",
        "    }\n",
        "    train_output, test_output = proprocess_output(train_output, test_output, class_mapping =  sarcasm_mapping)\n",
        "\n",
        "    template = \"Examine the input and categorize it as 'Sarcastic' or 'Non-Sarcastic' in the context of binary sarcasm detection: \"\n",
        "    train_features, test_features = prompt_eng(train_features, test_features, template)  # add the instructions and prompts\n",
        "    non_text_feature_modes = {'video': 'precomputed', 'audio': 'precomputed'}\n",
        "\n",
        "    # prepare model\n",
        "    tokenizer = T5Tokenizer.from_pretrained(LM_VERSION)\n",
        "    model = TextFeatureOPTModel(LM_VERSION, list(non_text_feature_modes.keys()), tokenizer, feature_modes=non_text_feature_modes).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    num_epochs = 3\n",
        "    # train_model(model, train_features, train_output, optimizer, criterion, device, num_epochs, checkpoint_path = 'checkpoints/')\n",
        "    accuracy = evaluate_model(model, test_features, test_output, criterion, device)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    config = CONFIG_BY_KEY[\"tav\"]\n",
        "    \n",
        "    print(\"Before running\")\n",
        "    gpu_monitor()\n",
        "    \n",
        "    data = DataPreper(config)\n",
        "    train(config, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g_iXThAuWnql"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     text_input = [\"A beautiful sunset over the mountains.\", \"Delicious food at a local restaurant.\"]\n",
        "#     sample_images = torch.rand(2, 3, 224, 224).to(device)\n",
        "\n",
        "#     # sample_audio = torch.rand(2, 1, audio_length).to(device)  # Move audio tensors to the specified device\n",
        "#     feature_data = {\n",
        "#         'video': sample_images,\n",
        "#         # 'audio': sample_audio\n",
        "#     }\n",
        "#     feature_types = list(feature_data.keys())\n",
        "#     features = list(feature_data.values())\n",
        "\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(LM_VERSION, use_fast=False)\n",
        "\n",
        "#     # Tokenize the text input\n",
        "#     text_input_ids = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
        "\n",
        "#     # Create an instance of TextFeatureOPTModel\n",
        "#     text_feature_opt_model = TextFeatureOPTModel(LM_VERSION, feature_types, tokenizer).to(device)\n",
        "\n",
        "#     # Perform inference with both image and audio features\n",
        "#     outputs = text_feature_opt_model(text_input_ids, features)\n",
        "#     print(outputs)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7DHNJmWkwFa"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# from train.info import *\n",
        "\n",
        "class MMIDataset(Dataset):\n",
        "    # Multi-modal Individual Dataset\n",
        "    def __init__(self, data_type: str, dataset_name: str, dataset_rootdir: str = '../meta/',\n",
        "                 data_split=[0], nrows: int = -1, filter_dim_coordination=False, sample_frac: float = 1.0,\n",
        "                 slice_range: tuple = None):\n",
        "        eps = 1e-5\n",
        "        # assert dataset_name in ALL_DATASETS\n",
        "        # assert data_type in ALL_DATA_TYPES\n",
        "        self.dataset_name = dataset_name\n",
        "        print(f\"Loading Dataset {dataset_name},\", end=' ')\n",
        "\n",
        "        if dataset_name in ['vreed_av']:\n",
        "            dataset_rootdir = dataset_rootdir.replace('meta', 'meta_vreed')\n",
        "        elif dataset_name in ['iemocap_arousal', 'iemocap_valence']:\n",
        "            dataset_rootdir = dataset_rootdir.replace('meta', 'meta_iemocap')\n",
        "\n",
        "        df_list = [pd.read_csv(dataset_rootdir + dataset_name + f'_{idx}_{data_type}.csv')\n",
        "                   for idx in data_split] if nrows <= 0 else \\\n",
        "                  [pd.read_csv(dataset_rootdir + dataset_name + f'_{idx}_{data_type}.csv', nrows=nrows)\n",
        "                   for idx in data_split]\n",
        "\n",
        "        df = pd.concat(df_list, ignore_index=True)\n",
        "        df = df[slice_range[0]:slice_range[1]] if slice_range is not None else df\n",
        "        df = df.sample(frac=sample_frac, random_state=1706)\n",
        "\n",
        "        features = [ft for ft in df.columns if not (ft.startswith('meta') or ft == 'y')]\n",
        "        data = df[features]\n",
        "        data = data.dropna(axis='columns')\n",
        "        data = (data - data.mean()) / (data.std() + eps)\n",
        "\n",
        "        modalities = list(set([ft.split('_')[0] for ft in data.columns]))\n",
        "\n",
        "        self.data = {}\n",
        "        self.feature_size = {}\n",
        "        self.all_modalities = []\n",
        "        for modality in modalities:\n",
        "            feature_names = [ft for ft in data.columns if ft.startswith(modality)]\n",
        "            feature_df = data[feature_names]\n",
        "            data_mod = torch.tensor(feature_df.values).float()\n",
        "            # if filter_dim_coordination:\n",
        "            #     if len(data_mod[0]) != MODALITY_FEATURE_SIZE[modality]:\n",
        "            #         continue\n",
        "            self.data[modality] = data_mod\n",
        "            self.feature_size[modality] = len(self.data[modality][0])\n",
        "            self.all_modalities.append(modality)\n",
        "\n",
        "        label = torch.tensor(df[['y']].values)\n",
        "        # self.task_type = DATASET_TASK[dataset_name]\n",
        "        self.task_type = 'C'\n",
        "        if self.task_type == 'C':\n",
        "            self.label = F.one_hot(label, num_classes=4).float().squeeze(dim=-2)\n",
        "            self.label_std = None\n",
        "        else:\n",
        "            self.label = (label - label.mean()) / (label.std() + eps)\n",
        "            self.label_std = label.std().item()\n",
        "            print(f\"label std: {self.label_std},\", end=' ')\n",
        "        self.dataset_size = len(self.label)\n",
        "        print(f\"size: {self.dataset_size}\")\n",
        "        if self.task_type == 'C':\n",
        "            # self.class_num = DATASET_CLASS_NUMBER[dataset_name]\n",
        "            self.class_num = 4\n",
        "        else:\n",
        "            self.class_num = None\n",
        "        print(self.label)\n",
        "        print()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.label[idx]\n",
        "\n",
        "        feature = {}\n",
        "        for mod in self.all_modalities:\n",
        "            feature[mod] = self.data[mod][idx]\n",
        "\n",
        "        return feature, label, self.dataset_name, self.task_type\n",
        "\n",
        "\n",
        "def get_datasets(args) -> Dict[str, MMIDataset]:\n",
        "    # assert args.dataset_name in ALL_DATASETS\n",
        "    assert not args.multitask\n",
        "\n",
        "    return {data_type: MMIDataset(data_type, args.dataset_name, args.dataset_dir, args.dataset_split)\n",
        "            for data_type in ALL_DATA_TYPES}\n",
        "\n",
        "\n",
        "def get_multitask_datasets(args) -> Tuple[Dict[str, List[MMIDataset]], Dict[str, MMIDataset]]:\n",
        "    assert args.multitask\n",
        "    # assert all(dataset_name in ALL_DATASETS for dataset_name in args.dataset_name_list)\n",
        "\n",
        "    print(\"\\n[Loading validation datasets]\")\n",
        "    multitask_validation_datasets = {dataset_name: MMIDataset('validation', dataset_name, args.dataset_dir, args.dataset_split)\n",
        "                                     for dataset_name in args.dataset_name_list}\n",
        "\n",
        "    print(\"\\n[Loading training datasets]\")\n",
        "    multitask_training_datasets = {dataset_name: [] for dataset_name in args.dataset_name_list}\n",
        "\n",
        "    if args.balanced:\n",
        "        for dataset_name in args.dataset_name_list:\n",
        "            start_idx = 0\n",
        "            end_idx = args.per_dataset_size\n",
        "            for _ in range(args.max_num):\n",
        "                try:\n",
        "                    dataset = MMIDataset('training', dataset_name, args.dataset_dir, args.dataset_split,\n",
        "                                         slice_range=(start_idx, end_idx))\n",
        "                except:\n",
        "                    break\n",
        "                multitask_training_datasets[dataset_name].append(dataset)\n",
        "\n",
        "                start_idx = end_idx\n",
        "                end_idx += args.per_dataset_size\n",
        "\n",
        "    else:\n",
        "        NotImplementedError()\n",
        "\n",
        "    return multitask_training_datasets, multitask_validation_datasets\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
